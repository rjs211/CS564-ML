{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries.\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import operator\n",
    "\n",
    "random.seed(11)\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_tags_count_dict = dict()\n",
    "# word_tag_dict = dict()\n",
    "# tag_tag_dict = dict()\n",
    "\n",
    "# def parse_sentence(sentence):\n",
    "#     previous = \"<s>\"\n",
    "#     previous = previous.strip()\n",
    "#     if previous not in unique_tags_count_dict:\n",
    "#         unique_tags_count_dict[previous] = 0\n",
    "#     unique_tags_count_dict[previous] += 1\n",
    "\n",
    "#     word_tags = sentence.split(\" \")\n",
    "\n",
    "#     for i, word_tag in enumerate(word_tags):\n",
    "#         word_tag = word_tag.strip()\n",
    "#         var_array = word_tag.split(\"/\")\n",
    "#         tag = var_array[-1]\n",
    "#         tag = tag.strip()\n",
    "\n",
    "#         if tag not in unique_tags_count_dict:\n",
    "#             unique_tags_count_dict[tag] = 0\n",
    "#         unique_tags_count_dict[tag] += 1\n",
    "\n",
    "#         if word_tag not in word_tag_dict:\n",
    "#             word_tag_dict[word_tag] = 0\n",
    "#         word_tag_dict[word_tag] += 1\n",
    "\n",
    "#         if previous + \"/\" + tag not in tag_tag_dict:\n",
    "#             tag_tag_dict[previous + \"/\" + tag] = 0\n",
    "#         tag_tag_dict[previous + \"/\" + tag] += 1\n",
    "\n",
    "#         previous = tag\n",
    "\n",
    "#     if previous + \"/<~s>\" not in tag_tag_dict:\n",
    "#         tag_tag_dict[previous + \"/<~s>\"] = 0\n",
    "#     tag_tag_dict[previous + \"/<~s>\"] += 1\n",
    "\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    word_tag_pairs = sentence.split(\" \")\n",
    "    words = []\n",
    "    tags = []\n",
    "    for i, word_tag in enumerate(word_tag_pairs):\n",
    "        word, tag = word_tag.strip().rsplit('/', 1)\n",
    "        words.append(word)\n",
    "        tags.append(tag)\n",
    "        \n",
    "    return words, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sentences = []\n",
    "with open('./Brown_train.txt', 'r') as file:\n",
    "    sentences = file.readlines()\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        parsed_sentences.append(parse_sentence(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=3, shuffle=False)\n",
    "parsed_sentences = np.asarray(parsed_sentences)\n",
    "for train_index, test_index in kf.split(parsed_sentences):\n",
    "    train_data = parsed_sentences[train_index]\n",
    "    test_data = parsed_sentences[test_index]\n",
    "    X_train = [a[0] for a in train_data]\n",
    "    Y_train = [a[1] for a in train_data]\n",
    "    X_test = [a[0] for a in test_data]\n",
    "    Y_test = [a[1] for a in test_data]\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(X_train, Y_train):\n",
    "    vocabulary2id = dict()\n",
    "    \n",
    "    tag2id = dict()\n",
    "    vocabulary2id['UNK'] = 0\n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            if word not in vocabulary2id.keys():\n",
    "                vocabulary2id[word] = len(vocabulary2id)\n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            if tag not in tag2id.keys():\n",
    "                tag2id[tag] = len(tag2id)\n",
    "    \n",
    "    return vocabulary2id, tag2id\n",
    "\n",
    "def get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id):\n",
    "    wordcount = defaultdict(int)\n",
    "    tagcount = defaultdict(int)\n",
    "    tagpaircount = defaultdict(int)\n",
    "    tagtriplecount = defaultdict(int)\n",
    "    \n",
    "    for sent in X_train:\n",
    "        for word in sent:\n",
    "            wordcount[word] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for tag in sent:\n",
    "            tagcount[tag] += 1\n",
    "    \n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 1):\n",
    "            tagpaircount[sent[i], sent[i + 1]] += 1\n",
    "\n",
    "    for sent in Y_train:\n",
    "        for i in range(len(sent) - 2):\n",
    "            tagtriplecount[sent[i], sent[i + 1], sent[i + 2]] += 1\n",
    "    \n",
    "    return wordcount, tagcount, tagpaircount, tagtriplecount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary2id, tag2id = get_vocab(X_train, Y_train)\n",
    "wordcount, tagcount, tagpaircount, tagtriplecount = get_word_tag_counts(X_train, Y_train, vocabulary2id, tag2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"UNK\"  # token to map all out-of-vocabulary words (OOVs)\n",
    "UNKid = 0      # index for UNK\n",
    "epsilon=1e-100\n",
    "array, ones, zeros, multiply, unravel_index = np.array, np.ones, np.zeros, np.multiply, np.unravel_index\n",
    "\n",
    "class HMM:\n",
    "        def __init__(self, state_list, observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None, \n",
    "                 smoothing_obs = 0.01, \n",
    "                 transition_proba1= None,\n",
    "                 prob_abs= 0.00001):\n",
    "            \"\"\"\n",
    "            Builds a Hidden Markov Model\n",
    "            * state_list is the list of state symbols [q_0...q_(N-1)]\n",
    "            * observation_list is the list of observation symbols [v_0...v_(M-1)]\n",
    "            * transition_proba is the transition probability matrix\n",
    "                [a_ij] a_ij,a_ik = Pr(Y_(t+1)=q_i|Y_t=q_j,Y_(t-1)=q_k)\n",
    "            * observation_proba is the observation probablility matrix\n",
    "                [b_ki] b_ki = Pr(X_t=v_k|Y_t=q_i)\n",
    "            * initial_state_proba is the initial state distribution\n",
    "                [pi_i] pi_i = Pr(Y_0=q_i)\"\"\"\n",
    "            \n",
    "            self.N = len(state_list)       # number of states\n",
    "            self.M = len(observation_list) # number of possible emissions\n",
    "            self.prob_abs = prob_abs\n",
    "            \n",
    "            self.omega_Y = state_list\n",
    "            self.omega_X = observation_list\n",
    "            if transition_proba1 is None:\n",
    "                self.transition_proba1 = zeros( (self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba1=transition_proba1\n",
    "            if transition_proba is None:\n",
    "                self.transition_proba = zeros( (self.N, self.N, self.N), float) \n",
    "            else:\n",
    "                self.transition_proba=transition_proba\n",
    "            if observation_proba is None:\n",
    "                self.observation_proba = zeros( (self.M, self.N), float) \n",
    "            else:\n",
    "                self.observation_proba=observation_proba\n",
    "            if initial_state_proba is None:\n",
    "                self.initial_state_proba = zeros( (self.N,), float ) \n",
    "            else:\n",
    "                self.initial_state_proba=initial_state_proba\n",
    "            self.make_indexes() # build indexes, i.e the mapping between token and int\n",
    "            self.smoothing_obs = smoothing_obs \n",
    "        \n",
    "        def make_indexes(self):\n",
    "            \"\"\"Creates the reverse table that maps states/observations names\n",
    "            to their index in the probabilities array\"\"\"\n",
    "            self.Y_index = {}\n",
    "            for i in range(self.N):\n",
    "                self.Y_index[self.omega_Y[i]] = i\n",
    "            self.X_index = {}\n",
    "            for i in range(self.M):\n",
    "                self.X_index[self.omega_X[i]] = i\n",
    "        \n",
    "        def get_observationIndices( self, observations ):\n",
    "            \"\"\"return observation indices, i.e \n",
    "            return [self.O_index[o] for o in observations]\n",
    "            and deals with OOVs\n",
    "            \"\"\"\n",
    "            indices = zeros( len(observations), int )\n",
    "            k = 0\n",
    "            for o in observations:\n",
    "                if o in self.X_index:\n",
    "                    indices[k] = self.X_index[o]\n",
    "                else:\n",
    "                    indices[k] = UNKid\n",
    "                k += 1\n",
    "            return indices\n",
    "\n",
    "        def data2indices(self, sent): \n",
    "            \"\"\"From one tagged sentence of the brown corpus: \n",
    "            - extract the words and tags \n",
    "            - returns two list of indices, one for each\n",
    "            -> (wordids, tagids)\n",
    "            \"\"\"\n",
    "            wordids = list()\n",
    "            tagids  = list()\n",
    "            for couple in sent:\n",
    "                wrd = couple[0]\n",
    "                tag = couple[1]\n",
    "                if wrd in self.X_index:\n",
    "                    wordids.append(self.X_index[wrd])\n",
    "                else:\n",
    "                    wordids.append(UNKid)\n",
    "                tagids.append(self.Y_index[tag])\n",
    "            return wordids,tagids\n",
    "        \n",
    "        def observation_estimation(self, pair_counts):\n",
    "            \"\"\" Build the observation distribution: \n",
    "                observation_proba is the observation probablility matrix\n",
    "                    [b_ki],  b_ki = Pr(X_t=v_k|Y_t=q_i)\"\"\"\n",
    "            # fill with counts\n",
    "            for pair in pair_counts:\n",
    "                wrd=pair[0]\n",
    "                tag=pair[1]\n",
    "                cpt=pair_counts[pair]\n",
    "                k = 0 # for <unk>\n",
    "                if wrd in self.X_index: \n",
    "                    k=self.X_index[wrd]\n",
    "                i=self.Y_index[tag]\n",
    "                self.observation_proba[k,i]=cpt\n",
    "            # normalize\n",
    "            self.observation_proba=self.observation_proba+self.smoothing_obs\n",
    "            self.observation_proba=self.observation_proba/self.observation_proba.sum(axis=0).reshape(1,self.N)\n",
    "\n",
    "        def transition_estimation(self, trans_counts):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j,Y_(t-1)=q_k)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for triple in trans_counts:\n",
    "                i=self.Y_index[triple[2]]\n",
    "                j=self.Y_index[triple[1]]\n",
    "                k=self.Y_index[triple[0]]\n",
    "                self.transition_proba[k,j,i]=trans_counts[triple]\n",
    "            # normalize sorun cıkacak !!!\n",
    "            self.transition_proba=self.transition_proba/self.transition_proba.sum(axis=0).reshape(self.N,self.N)\n",
    "\n",
    "        def transition_estimation1(self, trans_counts):\n",
    "            \"\"\" Build the transition distribution: \n",
    "                transition_proba is the transition matrix with : \n",
    "                [a_ij] a[i,j] = Pr(Y_(t+1)=q_i|Y_t=q_j)\n",
    "            \"\"\"\n",
    "            # fill with counts\n",
    "            for pair in trans_counts:\n",
    "                i=self.Y_index[pair[1]]\n",
    "                j=self.Y_index[pair[0]]\n",
    "                self.transition_proba1[j,i]=trans_counts[pair]\n",
    "            # normalize\n",
    "            self.transition_proba1=self.transition_proba1/self.transition_proba1.sum(axis=0).reshape(1,self.N)\n",
    "        \n",
    "        \n",
    "        def init_estimation(self, init_counts):\n",
    "            \"\"\"Build the init. distribution\"\"\"\n",
    "            # fill with counts\n",
    "            for tag in init_counts:\n",
    "                i=self.Y_index[tag]\n",
    "                self.initial_state_proba[i]=init_counts[tag]\n",
    "            # normalize\n",
    "            self.initial_state_proba=self.initial_state_proba/sum(self.initial_state_proba)\n",
    "             \n",
    "        \n",
    "        def supervised_training(self, pair_counts, trans_counts,init_counts,trans_counts1):\n",
    "            \"\"\" Train the HMM's parameters. This function wraps everything\"\"\"\n",
    "            self.observation_estimation(pair_counts)\n",
    "            self.transition_estimation(trans_counts)\n",
    "            self.transition_estimation1(trans_counts1)\n",
    "            self.init_estimation(init_counts)\n",
    "        \n",
    "        def viterbi(self,observations):\n",
    "            if len(observations)<2:\n",
    "                return [hmm.Y_index[z] for z in observations]\n",
    "            nSamples = len(observations)\n",
    "            nStates = self.transition_proba.shape[0] # number of states\n",
    "            c = np.zeros(nSamples) #scale factors (necessary to prevent underflow)\n",
    "            viterbi = np.zeros((nStates,nStates,nSamples)) # initialise viterbi table\n",
    "            viterbi1 = np.zeros((nStates,nSamples)) # initialise viterbi table\n",
    "            psi = np.zeros((nStates,nStates,nSamples)) # initialise the best path table\n",
    "            best_path = np.zeros(nSamples); # this will be your output\n",
    "            idx0 = self.X_index[observations[0]]\n",
    "            idx1 = self.X_index[observations[1]]\n",
    "            viterbi1[:,0] = self.initial_state_proba.T * self.observation_proba[idx0,:].T\n",
    "\n",
    "            for s in range (0,nStates): # loop through the states @(t-2)\n",
    "                for v in range (0,nStates): # loop through the states @(t-1)\n",
    "                    viterbi[s,v,1] = viterbi1[s,0] * self.transition_proba1[s,v] * self.observation_proba[idx1,v]\n",
    "\n",
    "            psi[0] = 0;\n",
    "\n",
    "            for t in range(2,nSamples): # loop through time\n",
    "                idx = self.X_index[observations[t]]\n",
    "                for s in range (0,nStates): # loop through the states @(t-1)\n",
    "                    for v in range (0,nStates): # loop through the states @(t-1)\n",
    "                        self.transition_proba[np.isnan(self.transition_proba)] = self.prob_abs\n",
    "                        trans_p = viterbi[:,s,t-1] * self.transition_proba[:,s,v]\n",
    "                \n",
    "                        if(math.isnan(trans_p[0])):\n",
    "                            trans_p[0]=0\n",
    "\n",
    "                        psi[s,v,t], viterbi[s,v,t] = max(enumerate(trans_p), key=operator.itemgetter(1))\n",
    "                        viterbi[s,v,t] = viterbi[s,v,t]*self.observation_proba[idx,v]\n",
    "\n",
    "\n",
    "            cabbar = viterbi[:,:,nSamples-1]\n",
    "            best_path[nSamples-1] = unravel_index(cabbar.argmax(), cabbar.shape)[1]\n",
    "            best_path[nSamples-2] = unravel_index(cabbar.argmax(), cabbar.shape)[0]\n",
    "\n",
    "#             return best_path, nSamples, psi\n",
    "            for t in range(nSamples-3,-1,-1): # states of (last-1)th to 0th time step\n",
    "                best_path[t] = psi[int(round(best_path[t+1])), int(round(best_path[t+2])), t+2]\n",
    "\n",
    "            return best_path\n",
    "        \n",
    "        def fwd_bkw(self, observations):\n",
    "            observations = x\n",
    "            self = hmm\n",
    "\n",
    "            nStates = self.transition_proba.shape[0]\n",
    "            start_prob = self.initial_state_proba\n",
    "            trans_prob = self.transition_proba1.transpose()\n",
    "\n",
    "            emm_prob = self.observation_proba.transpose()\n",
    "\n",
    "            # forward part of the algorithm\n",
    "            fwd = []\n",
    "            f_prev = {}\n",
    "            for i, observation_i in enumerate(observations):\n",
    "                f_curr = {}\n",
    "                for st in range(nStates):\n",
    "                    if i == 0:\n",
    "                        # base case for the forward part\n",
    "                        prev_f_sum = start_prob[st]\n",
    "                    else:\n",
    "                        prev_f_sum = sum(f_prev[k] * trans_prob[k][st] for k in range(nStates))\n",
    "\n",
    "                    f_curr[st] = emm_prob[st][self.X_index[observation_i]] * prev_f_sum\n",
    "\n",
    "                fwd.append(f_curr)\n",
    "                f_prev = f_curr\n",
    "\n",
    "            p_fwd = sum(f_curr[k] for k in range(nStates))\n",
    "\n",
    "            # backward part of the algorithm\n",
    "            bkw = []\n",
    "            b_prev = {}\n",
    "            for i, observation_i_plus in enumerate(reversed(observations[1:] + [None,])):\n",
    "                b_curr = {}\n",
    "                for st in range(nStates):\n",
    "                    if i == 0:\n",
    "                        # base case for backward part\n",
    "                        b_curr[st] = 1.0\n",
    "                    else:\n",
    "                        b_curr[st] = sum(trans_prob[st][l] * emm_prob[l][self.X_index[observation_i_plus]] * b_prev[l] for l in range(nStates))\n",
    "\n",
    "                bkw.insert(0,b_curr)\n",
    "                b_prev = b_curr\n",
    "\n",
    "            p_bkw = sum(start_prob[l] * emm_prob[l][self.X_index[observations[0]]] * b_curr[l] for l in range(nStates))\n",
    "\n",
    "            # merging the two parts\n",
    "            posterior = []\n",
    "            for i in range(len(observations)):\n",
    "                posterior.append({st: fwd[i][st] * bkw[i][st] / p_fwd for st in range(nStates)})\n",
    "\n",
    "            assert abs(p_fwd - p_bkw) < 1e-6\n",
    "            return fwd, bkw, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_counts(X, Y):\n",
    "    \"\"\" \n",
    "    Build different count tables to train a HMM. Each count table is a dictionnary. \n",
    "    Returns: \n",
    "    * c_words: word counts\n",
    "    * c_tags: tag counts\n",
    "    * c_pairs: count of pairs (word,tag)\n",
    "    * c_transitions: count of tag bigram \n",
    "    * c_inits: count of tag found in the first position\n",
    "    \"\"\"\n",
    "    c_words = dict()\n",
    "    c_tags = dict()\n",
    "    c_pairs= dict()\n",
    "    c_transitions = dict()\n",
    "    c_inits = dict()\n",
    "    c_transitions1 = dict()\n",
    "    \n",
    "    for sent in zip(X, Y):\n",
    "        # we use i because of the transition counts\n",
    "        sent = list(zip(*sent))\n",
    "        for i in range(len(sent)):\n",
    "            couple = sent[i]\n",
    "            wrd = couple[0]\n",
    "            tag = couple[1]\n",
    "            # word counts\n",
    "            if wrd in c_words:\n",
    "                c_words[wrd]=c_words[wrd]+1\n",
    "            else:\n",
    "                c_words[wrd]=1\n",
    "            # tag counts\n",
    "            if tag in c_tags:\n",
    "                c_tags[tag]=c_tags[tag]+1\n",
    "            else:\n",
    "                c_tags[tag]=1\n",
    "            # observation counts\n",
    "            if couple in c_pairs:\n",
    "                c_pairs[couple]=c_pairs[couple]+1\n",
    "            else:\n",
    "                c_pairs[couple]=1\n",
    "            # i >  0 -> transition counts\n",
    "            # j never is seen at second position\n",
    "            if i >= 1:\n",
    "                trans1 = (sent[i-1][1],tag)\n",
    "                if trans1 in c_transitions1:\n",
    "                    c_transitions1[trans1]=c_transitions1[trans1]+1\n",
    "                else:\n",
    "                    c_transitions1[trans1]=1\n",
    "            \n",
    "            if i > 1:\n",
    "                trans = (sent[i-2][1],sent[i-1][1],tag)\n",
    "                if trans in c_transitions:\n",
    "                    c_transitions[trans]=c_transitions[trans]+1\n",
    "                else:\n",
    "                    c_transitions[trans]=1\n",
    "            # i == 0 -> counts for initial states\n",
    "            else:\n",
    "                if tag in c_inits:\n",
    "                    c_inits[tag]=c_inits[tag]+1\n",
    "                else:\n",
    "                    c_inits[tag]=1\n",
    "    \n",
    "    return c_words,c_tags,c_pairs, c_transitions, c_inits, c_transitions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwords, ctags, cpairs, ctrans, cinits, ctrans1 = make_counts(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/btech/cse/2016/mukuntha.cs16/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:123: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "state_list = list(ctags.keys())\n",
    "observation_list = [a[0] for a in sorted(vocabulary2id.items(), key=lambda x: x[1])]\n",
    "hmm = HMM(state_list=state_list, observation_list=observation_list,\n",
    "                 transition_proba = None,\n",
    "                 observation_proba = None,\n",
    "                 initial_state_proba = None,\n",
    "                 smoothing_obs = 0.4,\n",
    "                 prob_abs = 0)\n",
    "hmm.supervised_training(cpairs,ctrans,cinits,ctrans1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for x, y_true in zip(X_test, Y_test):\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in vocabulary2id.keys():\n",
    "            x[i] = 'UNK'\n",
    "    pred_idx = hmm.viterbi(x)\n",
    "    y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "    y_true = np.asarray(y_true)\n",
    "    scores += (y_pred == y_true).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8308295872496935\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}'.format(np.asarray(scores).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y_true = X_train[0], Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stud/btech/cse/2016/mukuntha.cs16/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:249: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for x, y_true in zip(X_test, Y_test):\n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in vocabulary2id.keys():\n",
    "            x[i] = 'UNK'\n",
    "    pred_probs = hmm.fwd_bkw(x)\n",
    "    pred_idx = [max(probs.items(), key=lambda x: x[1])[0] for probs in pred_probs[2]]\n",
    "    y_pred = np.asarray([state_list[int(round(i))] for i in pred_idx])\n",
    "    y_true = np.asarray(y_true)\n",
    "    scores += (y_pred == y_true).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with only forward-backward: 0.7162525541479362\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy with only forward-backward: {}'.format(np.asarray(scores).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
