{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/stud/btech/cse/2016/rsrivatsa.cs16/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, random\n",
    "from copy import copy,deepcopy\n",
    "# Download nltk stopwords corpus.\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from nltk import FreqDist\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class FullyConnected():\n",
    "    layer_count = 0\n",
    "\n",
    "    def __init__(self, out_features=100, in_features=100):\n",
    "        '''\n",
    "            Fully Connected layer\n",
    "            out_features: No of Output Features\n",
    "            no_of_inp_featues: No of Input Features\n",
    "        '''\n",
    "        self.out_features = out_features\n",
    "        self.in_features = in_features\n",
    "        self.weight = np.random.normal(\n",
    "            loc=0.0, scale=1/np.sqrt(self.out_features), size=(self.out_features, self.in_features))\n",
    "        self.bias = np.zeros((self.out_features, 1), dtype=np.float32)\n",
    "        self.weight_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.layer_id = FullyConnected.layer_count\n",
    "        FullyConnected.layer_count += 1\n",
    "\n",
    "    def __str__(self, ):\n",
    "        return 'Fully Connected Layer ID = {}: (In:{}, Out:{})'.format(self.layer_id, self.in_features, self.out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "            x: Input (N, F_in) where N = No. of samples, F_in: No. of input features\n",
    "            \n",
    "            returns:\n",
    "                Wx + B (N, F_out) where W, B are trainable weight and bias where N: No. of samples, F_out Output dimension/features.\n",
    "        '''\n",
    "        assert x.shape[1] == self.in_features, \"Input dimension Mismatch\"\n",
    "        wx = np.matmul(self.weight, np.transpose(x, (1, 0)))\n",
    "        wx_plus_b = np.add(wx, self.bias)\n",
    "        return np.transpose(wx_plus_b, (1, 0))\n",
    "\n",
    "    def backward(self, y_grad, y, x):\n",
    "        '''\n",
    "            y_grad: Gradiant of output (N, F_out)\n",
    "            y: Output of layer (N, F_out)\n",
    "            x: input of layer (N, F_in)\n",
    "        '''\n",
    "        grad_x = np.matmul(y_grad, self.weight)\n",
    "\n",
    "        weight_grad = np.matmul(np.transpose(y_grad, (1, 0)), x)\n",
    "        if self.weight_grad is not None:\n",
    "          self.weight_grad += weight_grad\n",
    "        else:\n",
    "          self.weight_grad = weight_grad\n",
    "        assert y_grad.shape == y.shape, \"y_grad.shape: {}, y.shape: {}\".format(\n",
    "            y_grad.shape, y.shape)\n",
    "\n",
    "        bias_grad = np.sum(np.transpose(y_grad, (1, 0)), axis=1, keepdims=True)\n",
    "        assert bias_grad.shape == self.bias.shape, \"bias_grad.shape: {}, self.bias.shape: {}\".format(\n",
    "            bias_grad.shape, self.bias.shape)\n",
    "        if self.bias_grad is not None:\n",
    "          self.bias_grad += bias_grad\n",
    "        else:\n",
    "          self.bias_grad = bias_grad\n",
    "        return grad_x\n",
    "\n",
    "    def apply_gradients(self, lr=0.001):\n",
    "\n",
    "        self.weight -= lr * self.weight_grad\n",
    "        self.bias -= lr * self.bias_grad\n",
    "\n",
    "        convergence_criteria = np.sum(np.absolute(\n",
    "            self.weight_grad)) + np.sum(np.absolute(self.bias_grad))\n",
    "        self.weight_grad = None\n",
    "        self.bias_grad = None\n",
    "        # deb(convergence_criteria)\n",
    "\n",
    "        if convergence_criteria < 1e-3:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "\n",
    "class SoftmaxCrossEntropyLoss():\n",
    "  \"\"\"\n",
    "    Combines a softmax layer with the cross-entropy loss\n",
    "  \"\"\"\n",
    "  layer_count = 0\n",
    "\n",
    "  def __init__(self):\n",
    "      self.layer_id = SoftmaxCrossEntropyLoss.layer_count\n",
    "      SoftmaxCrossEntropyLoss.layer_count += 1\n",
    "\n",
    "  def __str__(self, ):\n",
    "      return 'Softmax Cross Entropy Loss Layer ID = {}'.format(self.layer_id)\n",
    "\n",
    "  def forward(self, x, y_true):\n",
    "    '''\n",
    "      x: Input (N, C) where N = No. of samples, C = No. of classes\n",
    "      y_true: Target (N, C), boolean values\n",
    "    '''\n",
    "    # Softmax\n",
    "    x_stable = x - np.max(x, axis=1, keepdims=True)\n",
    "    p = np.exp(x_stable) / np.sum(np.exp(x_stable), axis=1, keepdims=True)\n",
    "\n",
    "    # Cross Entropy Loss\n",
    "    loss = y_true * \\\n",
    "        (- x_stable + np.log(np.sum(np.exp(x_stable), axis=1, keepdims=True)))\n",
    "    loss = np.mean(np.sum(loss, axis=-1))\n",
    "    return p, loss\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    '''\n",
    "      y_pred: (N, C)\n",
    "      y_true: (N, C)\n",
    "    '''\n",
    "    # Using y_pred.shape[0] as a workaround for batch_size.\n",
    "    return (y_pred - y_true) / y_pred.shape[0]\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.forward(*args, **kwargs)\n",
    "\n",
    "  def apply_gradients(self, lr=None):\n",
    "    return True\n",
    "\n",
    "\n",
    "class Sigmoid():\n",
    "  \"\"\"\n",
    "  Class for implementing sigmoid layer.\n",
    "  \"\"\"\n",
    "  layer_count = 0\n",
    "\n",
    "  def __init__(self):\n",
    "    self.layer_id = Sigmoid.layer_count\n",
    "    Sigmoid.layer_count += 1\n",
    "\n",
    "  def __str__(self, ):\n",
    "      return 'Sigmoid Layer ID = {}'.format(self.layer_id)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Function for implemeting the sigmoid expression.\n",
    "\n",
    "    x: Input(N, C) where N = number of samples, C = number of classes.\n",
    "    \"\"\"\n",
    "    y = 1.0 / (1.0 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "  def backward(self, y_grad, y, x):\n",
    "    \"\"\"\n",
    "    Function for calculating the gradients of the current sigmoid perceptron.\n",
    "\n",
    "    y_grad: Gradient at the output.\n",
    "    y: The calculated forward(x).\n",
    "    \"\"\"\n",
    "    w_grad = y_grad * y * (1 - y)\n",
    "    return w_grad\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.forward(*args, **kwargs)\n",
    "\n",
    "  def apply_gradients(self, lr=None):\n",
    "    return True\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "  \"\"\"\n",
    "  Class for implementing tanh layer.\n",
    "  \"\"\"\n",
    "  layer_count = 0\n",
    "\n",
    "  def __init__(self):\n",
    "    self.layer_id = Tanh.layer_count\n",
    "    Tanh.layer_count += 1\n",
    "\n",
    "  def __str__(self, ):\n",
    "      return 'Tanh Layer ID = {}'.format(self.layer_id)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Function for implemeting the tanh expression.\n",
    "\n",
    "    x: Input(N, C) where N = number of samples, C = number of classes.\n",
    "    \"\"\"\n",
    "    y = np.tanh(x)\n",
    "    return y\n",
    "\n",
    "  def backward(self, y_grad, y, x):\n",
    "    \"\"\"\n",
    "    Function for calculating the gradients of the current sigmoid perceptron.\n",
    "\n",
    "    y_grad: Gradient at the output.\n",
    "    y: The calculated forward(x).\n",
    "    \"\"\"\n",
    "    w_grad = y_grad * (1 - (y ** 2))\n",
    "    return w_grad\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.forward(*args, **kwargs)\n",
    "\n",
    "  def apply_gradients(self, lr=None):\n",
    "    return True\n",
    "\n",
    "\n",
    "class SoftmaxLayer():\n",
    "  \"\"\"\n",
    "  Class implementing a softmax layer\n",
    "  \"\"\"\n",
    "  layer_count = 0\n",
    "\n",
    "  def __init__(self):\n",
    "    self.layer_id = SoftmaxLayer.layer_count\n",
    "    SoftmaxLayer.layer_count += 1\n",
    "\n",
    "  def __str__(self, ):\n",
    "      return 'Softmax Layer ID = {}'.format(self.layer_id)\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      x: Input (N, C) where N = No. of samples, C = No. of classes\n",
    "    '''\n",
    "    # Softmax\n",
    "    x_stable = x - np.max(x, axis=1, keepdims=True)\n",
    "    p = np.exp(x_stable) / np.sum(np.exp(x_stable), axis=1, keepdims=True)\n",
    "\n",
    "    return p\n",
    "\n",
    "  def backward(self, y_grad, y, x):\n",
    "    '''\n",
    "    y_grad: Gradient at output\n",
    "    y: (N, C)\n",
    "    '''\n",
    "    raise NotImplementedError(\n",
    "        \"Backprop not implemented for SoftmaxLayer, please use SoftmaxCrossEntropyLoss instead.\")\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.forward(*args, **kwargs)\n",
    "\n",
    "  def apply_gradients(self, lr=None):\n",
    "    return True\n",
    "\n",
    "\n",
    "class SigmoidCrossEntropyLoss():\n",
    "  \"\"\"\n",
    "    Combines a sigmoid layer with the cross-entropy loss.\n",
    "  \"\"\"\n",
    "  layer_count = 0\n",
    "\n",
    "  def __init__(self):\n",
    "    self.layer_id = SigmoidCrossEntropyLoss.layer_count\n",
    "    SigmoidCrossEntropyLoss.layer_count += 1\n",
    "\n",
    "  def __str__(self, ):\n",
    "    return 'Softmax Cross Entropy Loss Layer ID = {}'.format(self.layer_id)\n",
    "\n",
    "  def forward(self, x, y_true):\n",
    "    '''\n",
    "      x: Input (N, C) where N = No. of samples, C = No. of classes\n",
    "      y_true: Target (N, C), boolean values\n",
    "    '''\n",
    "    # Sigmoid\n",
    "    p = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    # Cross Entropy Loss\n",
    "    x_stable = np.maximum(x, 0)\n",
    "    loss = x_stable - x * y_true + np.log(1 + np.exp(np.absolute(x)))\n",
    "    loss = np.mean(np.sum(loss, axis=-1))\n",
    "    return p, loss\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    '''\n",
    "      y_pred: (N, C)\n",
    "      y_true: (N, C)\n",
    "    '''\n",
    "    # Using y_pred.shape[0] as a workaround for batch_size.\n",
    "    return (y_pred - y_true) / y_pred.shape[0]\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.forward(*args, **kwargs)\n",
    "\n",
    "  def apply_gradients(self, lr=None):\n",
    "    return True\n",
    "\n",
    "\n",
    "class SigmoidMSELoss():\n",
    "  \"\"\"\n",
    "    Combines a sigmoid layer with the mean squared error loss.\n",
    "  \"\"\"\n",
    "  layer_count = 0\n",
    "\n",
    "  def __init__(self):\n",
    "    self.layer_id = SigmoidMSELoss.layer_count\n",
    "    SigmoidMSELoss.layer_count += 1\n",
    "\n",
    "  def __str__(self, ):\n",
    "      return 'Sigmoid Mean Squared Error Loss Layer ID = {}'.format(self.layer_id)\n",
    "\n",
    "  def forward(self, x, y_true):\n",
    "    '''\n",
    "      x: Input (N, C) where N = No. of samples, C = No. of classes\n",
    "      y_true: Target (N, C), boolean values\n",
    "    '''\n",
    "    # Sigmoid\n",
    "    p = 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    # Mean Squared Error Loss\n",
    "    squared_error = (y_true - p) ** 2\n",
    "    assert len(squared_error.shape) == 2\n",
    "    loss = (np.sum(squared_error, axis=-1)) / 2\n",
    "    loss = np.mean(loss, axis=-1)\n",
    "    return p, loss\n",
    "\n",
    "  def backward(self, y_pred, y_true):\n",
    "    '''\n",
    "      y_pred: (N, C)\n",
    "      y_true: (N, C)\n",
    "    '''\n",
    "    # Using y_pred.shape[0] as a workaround for batch_size.\n",
    "    return -(y_true - y_pred) * y_pred * (1 - y_pred) / y_pred.shape[0]\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.forward(*args, **kwargs)\n",
    "\n",
    "  def apply_gradients(self, lr=None):\n",
    "    return True\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron():\n",
    "  def __init__(self, layers, loss_layer, final_layer):\n",
    "    self.layers = layers\n",
    "    self.loss_layer = loss_layer\n",
    "    self.final_layer = final_layer\n",
    "    self.all_outputs = []\n",
    "    self.train = True\n",
    "\n",
    "  def __str__(self, ):\n",
    "      summary = []\n",
    "      for layer in self.layers:\n",
    "          summary.append(str(layer))\n",
    "      if self.final_layer is not None:\n",
    "          summary.append('Final Layer: ' + str(self.final_layer))\n",
    "      if self.loss_layer is not None:\n",
    "        summary.append('Loss Layer: ' + str(self.loss_layer))\n",
    "      return ' \\n'.join(summary)\n",
    "\n",
    "  def forward(self, x, y_true=None):\n",
    "    # Training mode\n",
    "    if self.train:\n",
    "      assert (self.loss_layer is not None) and (\n",
    "          y_true is not None), \"Training mode, please pass y_true and set a loss layer\"\n",
    "      all_outputs = [x]\n",
    "\n",
    "      for layer in self.layers:\n",
    "        #print(layer, x.shape, end=', ')\n",
    "        x = layer(x)\n",
    "        all_outputs.append(x)\n",
    "        #print(x.shape)\n",
    "        # deb(layer)\n",
    "\n",
    "      pred, loss = self.loss_layer(x, y_true)\n",
    "      all_outputs.append((pred, y_true))\n",
    "\n",
    "      # deb(len(all_outputs))\n",
    "      # Save outputs for multiple forward passes\n",
    "      self.all_outputs.append(all_outputs)\n",
    "\n",
    "    # Evaluation mode\n",
    "    else:\n",
    "      assert (self.final_layer is not None) and (\n",
    "          y_true is None), \"Evaluation mode, doesn't take y_true as input\"\n",
    "      all_outputs = [x]\n",
    "\n",
    "      for layer in self.layers:\n",
    "        x = layer(x)\n",
    "        all_outputs.append(x)\n",
    "\n",
    "      pred = self.final_layer(x)\n",
    "      loss = None\n",
    "      all_outputs.append((pred, y_true))\n",
    "\n",
    "    return all_outputs, loss\n",
    "\n",
    "  def backward(self):\n",
    "      assert self.train\n",
    "\n",
    "      for all_outputs in self.all_outputs[::-1]:\n",
    "        y_grad = self.loss_layer.backward(*all_outputs[-1])\n",
    "        all_outputs = all_outputs[:-1]\n",
    "        for idx, layer in enumerate(self.layers[::-1]):\n",
    "          # Pass the grad at the output, output of the layer, the input of the layer to backward()\n",
    "          y_grad = layer.backward(\n",
    "              y_grad, all_outputs[-idx-1], all_outputs[-idx-2])\n",
    "\n",
    "      self.all_outputs = []\n",
    "      return\n",
    "\n",
    "  def train_mode(self):\n",
    "      '''\n",
    "      Set train mode\n",
    "      '''\n",
    "      self.train = True\n",
    "\n",
    "  def eval_mode(self):\n",
    "      '''\n",
    "      Set eval mode\n",
    "      '''\n",
    "      self.train = False\n",
    "\n",
    "  def optimize(self, learning_rate):\n",
    "      '''\n",
    "      Optimizes\n",
    "      '''\n",
    "      conv = True\n",
    "      for layer in self.layers:\n",
    "        conv = layer.apply_gradients(learning_rate) and conv\n",
    "      return conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tester(model, x, y_true):\n",
    "    model.eval_mode()\n",
    "    out, _ = model.forward(x)\n",
    "    model.train_mode()\n",
    "    predictions = out[-1][0] > 0.5\n",
    "    ground_truth = y_true > 0.5\n",
    "    acc = np.mean(predictions == ground_truth)\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(\n",
    "        ground_truth, predictions, average='weighted')\n",
    "    return acc, precision, recall, fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "deb = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the file line by line and clean the text of punctuation.\n",
    "\n",
    "with open('pos_sentiment.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "    data = [re.sub(r'([^\\w\\s]|[0-9])', ' ', line) for line in data]\n",
    "    data_pos = [re.sub(r'(\\s+)', ' ', line) for line in data]\n",
    "\n",
    "with open('neg_sentiment.txt', 'r') as file:\n",
    "    data = file.readlines()\n",
    "    data = [re.sub(r'([^\\w\\s]|[0-9])', ' ', line) for line in data]\n",
    "    data_neg = [re.sub(r'(\\s+)', ' ', line) for line in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pos = [1]*len(data_pos)\n",
    "y_neg= [0]*len(data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_x = data_pos + data_neg\n",
    "data_y = y_pos + y_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_x = [sent.strip().split() for sent in data_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('')\n",
    "\n",
    "data_x = [[w for w in words if w not in stop_words] for words in data_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Function for generating the bag-of-words vocabulary.\n",
    "def get_vocab(XData):\n",
    "    vocab = set()\n",
    "\n",
    "    for line in XData:\n",
    "        for word in line:\n",
    "            vocab.add(word)\n",
    "            \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "all_tokens = sum(data_x,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab =[a[0] for a in fd.most_common() if a[1] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7400"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word2idx = { w:i for i, w in enumerate(vocab) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vectorize(x_data, w2id):\n",
    "    x_vec = np.zeros((len(x_data), len(w2id)), dtype=np.float32)\n",
    "    for i,sent in enumerate(x_data):\n",
    "        for word in sent:\n",
    "            if word in w2id.keys():\n",
    "                x_vec[i, w2id[word]] += 1\n",
    "    return x_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_x_vec = vectorize(data_x, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_y = np.expand_dims(np.asarray(data_y, dtype=np.float32), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6662, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_val_idx, test_idx = next(StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=0).split(data_x_vec, data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_idx, valid_idx = next(StratifiedShuffleSplit(n_splits=1,test_size=0.2,random_state=0).split(train_val_idx,data_y[train_val_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_idx = train_val_idx[train_idx]\n",
    "valid_idx = train_val_idx[valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_train, x_val, x_test = data_x_vec[train_idx], data_x_vec[valid_idx], data_x_vec[test_idx]\n",
    "y_train, y_val, y_test = data_y[train_idx], data_y[valid_idx], data_y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layers = [\n",
    "    FullyConnected(48, 7400),\n",
    "    Tanh(),\n",
    "    FullyConnected(8, 48),\n",
    "    Tanh(),\n",
    "    FullyConnected(1, 8),\n",
    "]\n",
    "\n",
    "mymlp = MultiLayerPerceptron(layers, SigmoidCrossEntropyLoss(), Sigmoid())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully Connected Layer ID = 0: (In:7400, Out:48) \n",
      "Tanh Layer ID = 0 \n",
      "Fully Connected Layer ID = 1: (In:48, Out:8) \n",
      "Tanh Layer ID = 1 \n",
      "Fully Connected Layer ID = 2: (In:8, Out:1) \n",
      "Final Layer: Sigmoid Layer ID = 0 \n",
      "Loss Layer: Softmax Cross Entropy Loss Layer ID = 0\n"
     ]
    }
   ],
   "source": [
    "print(mymlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# losses_run = []\n",
    "# for epno in range(5000):\n",
    "#     mymlp.train_mode()\n",
    "#     # Take forward step\n",
    "#     _, loss = mymlp.forward(x_train, y_train)\n",
    "#     # Make backward pass accumulating gradients\n",
    "#     mymlp.backward()\n",
    "#     # Take an optimization step updating the weights with gradients \n",
    "#     _ = mymlp.optimize(0.475)\n",
    "#     # Print the loss\n",
    "#     print('Loss', loss)\n",
    "#     losses_run.append(loss)\n",
    "#     if epno % 5 == 0:\n",
    "#         mymlp.eval_mode()\n",
    "#         out, _ = mymlp.forward(x_val)\n",
    "#         out = (np.asarray(out) > 0.5)\n",
    "#         mymlp.train_mode()\n",
    "#         print('Accuracy {} : {}'.format(np.mean(out == (y_val > 0.5) ), epno)) \n",
    "# #         print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_acc: 0.6685432793807178, train_prec: 0.7189520310368555, train_rec: 0.6685432793807178, train_f1: 0.6482672850491638\n",
      "Epoch 0 val_acc: 0.6022514071294559, val_prec: 0.644415720002983, val_rec: 0.6022514071294559, val_f1: 0.5709333100439334\n",
      "Epoch 0 test_acc: 0.5918979744936234, test_prec: 0.6265236292935525, test_rec: 0.5918979744936234, test_f1: 0.5621476797770871\n",
      "Saving as best model...\n",
      "Loss 1.8296735334769134\n",
      "Epoch 1 train_acc: 0.8336851982172179, train_prec: 0.8369836876223256, train_rec: 0.8336851982172179, train_f1: 0.8332748108950534\n",
      "Epoch 1 val_acc: 0.6782363977485929, val_prec: 0.6824500590931366, val_rec: 0.6782363977485929, val_f1: 0.6763678366429308\n",
      "Epoch 1 test_acc: 0.6744186046511628, test_prec: 0.6775453329254992, test_rec: 0.6744186046511628, test_f1: 0.673009785916965\n",
      "Saving as best model...\n",
      "Loss 2.91241250402792\n",
      "Epoch 2 train_acc: 0.8669950738916257, train_prec: 0.8719869071465749, train_rec: 0.8669950738916257, train_f1: 0.8665452339816178\n",
      "Epoch 2 val_acc: 0.701688555347092, val_prec: 0.7087454687870011, val_rec: 0.701688555347092, val_f1: 0.6991458635085626\n",
      "Epoch 2 test_acc: 0.6781695423855963, test_prec: 0.6844957734205415, test_rec: 0.6781695423855963, test_f1: 0.6754286944044829\n",
      "Saving as best model...\n",
      "Loss 3.7463428312582203\n",
      "Epoch 3 train_acc: 0.7884119164907343, train_prec: 0.8393651449217552, train_rec: 0.7884119164907343, train_f1: 0.7801476212428363\n",
      "Epoch 3 val_acc: 0.6425891181988743, val_prec: 0.706800081682663, val_rec: 0.6425891181988743, val_f1: 0.6125104589337181\n",
      "Epoch 3 test_acc: 0.6406601650412603, test_prec: 0.7039510486837482, test_rec: 0.6406601650412603, test_f1: 0.6105827808973506\n",
      "Loss 4.306926284459132\n",
      "Epoch 4 train_acc: 0.8925639221205723, train_prec: 0.9001803245920759, train_rec: 0.8925639221205723, train_f1: 0.8920522510371456\n",
      "Epoch 4 val_acc: 0.6829268292682927, val_prec: 0.6969025291275931, val_rec: 0.6829268292682927, val_f1: 0.6771989048479115\n",
      "Epoch 4 test_acc: 0.68192048012003, test_prec: 0.6948397078616574, test_rec: 0.68192048012003, test_f1: 0.6765026261373697\n",
      "Loss 5.451910307055956\n",
      "Epoch 5 train_acc: 0.9232934553131598, train_prec: 0.9246179180078881, train_rec: 0.9232934553131598, train_f1: 0.9232341404407357\n",
      "Epoch 5 val_acc: 0.7148217636022514, val_prec: 0.7190439785652499, val_rec: 0.7148217636022514, val_f1: 0.7134408602150537\n",
      "Epoch 5 test_acc: 0.68192048012003, test_prec: 0.6859055173901181, test_rec: 0.68192048012003, test_f1: 0.6801740999160585\n",
      "Saving as best model...\n",
      "Loss 6.423002610708466\n",
      "Epoch 6 train_acc: 0.9242317616701853, train_prec: 0.925111362490779, train_rec: 0.9242317616701853, train_f1: 0.9241921079527812\n",
      "Epoch 6 val_acc: 0.7138836772983115, val_prec: 0.7150349650349651, val_rec: 0.7138836772983115, val_f1: 0.7135002004696718\n",
      "Epoch 6 test_acc: 0.6811702925731433, test_prec: 0.681743618862109, test_rec: 0.6811702925731433, test_f1: 0.6809311154811563\n",
      "Saving as best model...\n",
      "Loss 5.896906502535118\n",
      "Epoch 7 train_acc: 0.9479239971850809, train_prec: 0.9486202623305214, train_rec: 0.9479239971850809, train_f1: 0.9479035290684723\n",
      "Epoch 7 val_acc: 0.7138836772983115, val_prec: 0.7150349650349651, val_rec: 0.7138836772983115, val_f1: 0.7135002004696718\n",
      "Epoch 7 test_acc: 0.6901725431357839, test_prec: 0.691401188379756, test_rec: 0.6901725431357839, test_f1: 0.6896912604786102\n",
      "Loss 7.323006007890562\n",
      "Epoch 8 train_acc: 0.9540229885057471, train_prec: 0.9551069414526947, train_rec: 0.9540229885057471, train_f1: 0.9539958719678182\n",
      "Epoch 8 val_acc: 0.7148217636022514, val_prec: 0.7188146501496925, val_rec: 0.7148217636022514, val_f1: 0.7135148324039139\n",
      "Epoch 8 test_acc: 0.6984246061515379, test_prec: 0.7029605828857807, test_rec: 0.6984246061515379, test_f1: 0.6967005261340873\n",
      "Saving as best model...\n",
      "Loss 7.413306691492212\n",
      "Epoch 9 train_acc: 0.9481585737743373, train_prec: 0.9512008678423844, train_rec: 0.9481585737743373, train_f1: 0.9480715660722817\n",
      "Epoch 9 val_acc: 0.7082551594746717, val_prec: 0.7212130956298046, val_rec: 0.7082551594746717, val_f1: 0.7039193008935308\n",
      "Epoch 9 test_acc: 0.6856714178544636, test_prec: 0.6956644441571854, test_rec: 0.6856714178544636, test_f1: 0.6815577610394011\n",
      "Loss 8.51894880850074\n",
      "Epoch 10 train_acc: 0.9650480882007976, train_prec: 0.9650776066290382, train_rec: 0.9650480882007976, train_f1: 0.9650474996696671\n",
      "Epoch 10 val_acc: 0.701688555347092, val_prec: 0.7016913951810198, val_rec: 0.701688555347092, val_f1: 0.701687505280054\n",
      "Epoch 10 test_acc: 0.6886721680420105, test_prec: 0.6889467524921185, test_rec: 0.6886721680420105, test_f1: 0.6885670082536315\n",
      "Loss 7.549716607425201\n",
      "Epoch 11 train_acc: 0.9680975838611307, train_prec: 0.9693939387754863, train_rec: 0.9680975838611307, train_f1: 0.9680757450770915\n",
      "Epoch 11 val_acc: 0.6960600375234521, val_prec: 0.7035131629427992, val_rec: 0.6960600375234521, val_f1: 0.6932515773318934\n",
      "Epoch 11 test_acc: 0.6916729182295573, test_prec: 0.6979433796724576, test_rec: 0.6916729182295573, test_f1: 0.6891747972610397\n",
      "Loss 7.41551448756407\n",
      "Epoch 12 train_acc: 0.9826413323950269, train_prec: 0.9826684987539143, train_rec: 0.9826413323950269, train_f1: 0.9826410725816354\n",
      "Epoch 12 val_acc: 0.7035647279549718, val_prec: 0.7043964472154186, val_rec: 0.7035647279549718, val_f1: 0.703262861169838\n",
      "Epoch 12 test_acc: 0.6984246061515379, test_prec: 0.6988939155462104, test_rec: 0.6984246061515379, test_f1: 0.6982561518050229\n",
      "Loss 7.6524085860840305\n",
      "Epoch 13 train_acc: 0.9824067558057706, train_prec: 0.9824247260135344, train_rec: 0.9824067558057706, train_f1: 0.9824066047826968\n",
      "Epoch 13 val_acc: 0.7148217636022514, val_prec: 0.71497007675427, val_rec: 0.7148217636022514, val_f1: 0.7147725672440501\n",
      "Epoch 13 test_acc: 0.7036759189797449, test_prec: 0.703679044972292, test_rec: 0.7036759189797449, test_f1: 0.7036739177737091\n",
      "Saving as best model...\n",
      "Loss 8.345014134685808\n",
      "Epoch 14 train_acc: 0.9821721792165142, train_prec: 0.9821789872121263, train_rec: 0.9821721792165142, train_f1: 0.9821721242805447\n",
      "Epoch 14 val_acc: 0.7101313320825516, val_prec: 0.7101912622179657, val_rec: 0.7101313320825516, val_f1: 0.7101106686321532\n",
      "Epoch 14 test_acc: 0.6909227306826706, test_prec: 0.6912505454344462, test_rec: 0.6909227306826706, test_f1: 0.6907814221467046\n",
      "Loss 9.205148665588025\n",
      "Epoch 15 train_acc: 0.9856908280553601, train_prec: 0.9859074715899113, train_rec: 0.9856908280553601, train_f1: 0.985689268878806\n",
      "Epoch 15 val_acc: 0.7035647279549718, val_prec: 0.7071374588540379, val_rec: 0.7035647279549718, val_f1: 0.702280954805136\n",
      "Epoch 15 test_acc: 0.6969242310577645, test_prec: 0.7008888753528264, test_rec: 0.6969242310577645, test_f1: 0.6953933063844557\n",
      "Loss 6.69027422473143\n",
      "Epoch 16 train_acc: 0.9910860896082571, train_prec: 0.9911384322493827, train_rec: 0.9910860896082571, train_f1: 0.9910858629924952\n",
      "Epoch 16 val_acc: 0.7148217636022514, val_prec: 0.7159192626793328, val_rec: 0.7148217636022514, val_f1: 0.7144589184007218\n",
      "Epoch 16 test_acc: 0.6924231057764441, test_prec: 0.693175735075138, test_rec: 0.6924231057764441, test_f1: 0.6921101641689493\n",
      "Loss 7.860022797089677\n",
      "Epoch 17 train_acc: 0.9969505043396669, train_prec: 0.996951487765803, train_rec: 0.9969505043396669, train_f1: 0.9969505023260401\n",
      "Epoch 17 val_acc: 0.7073170731707317, val_prec: 0.7075537857029585, val_rec: 0.7073170731707317, val_f1: 0.7072335990535479\n",
      "Epoch 17 test_acc: 0.6969242310577645, test_prec: 0.6971465250033438, test_rec: 0.6969242310577645, test_f1: 0.6968454102843722\n",
      "Loss 9.622047726975149\n",
      "Epoch 18 train_acc: 0.9819376026272578, train_prec: 0.9825036855675221, train_rec: 0.9819376026272578, train_f1: 0.9819323771737696\n",
      "Epoch 18 val_acc: 0.7054409005628518, val_prec: 0.7142369459484262, val_rec: 0.7054409005628518, val_f1: 0.702386075476664\n",
      "Epoch 18 test_acc: 0.6849212303075769, test_prec: 0.6939253946953298, test_rec: 0.6849212303075769, test_f1: 0.6811744454135431\n",
      "Loss 9.845156987304545\n",
      "Epoch 19 train_acc: 0.9964813511611541, train_prec: 0.9964814608262771, train_rec: 0.9964813511611541, train_f1: 0.9964813511611541\n",
      "Epoch 19 val_acc: 0.7035647279549718, val_prec: 0.7035675942059595, val_rec: 0.7035647279549718, val_f1: 0.7035636844921292\n",
      "Epoch 19 test_acc: 0.700675168792198, test_prec: 0.70069494843846, test_rec: 0.700675168792198, test_f1: 0.7006657350040941\n",
      "Loss 11.021831434524355\n",
      "Epoch 20 train_acc: 0.9913206661975135, train_prec: 0.991338955240205, train_rec: 0.9913206661975135, train_f1: 0.9913205916927973\n",
      "Epoch 20 val_acc: 0.6951219512195121, val_prec: 0.6967850955667262, val_rec: 0.6951219512195121, val_f1: 0.6944764122033061\n",
      "Epoch 20 test_acc: 0.6991747936984246, test_prec: 0.7003306299093286, test_rec: 0.6991747936984246, test_f1: 0.6987251289525769\n",
      "Loss 10.875070281482765\n",
      "Epoch 21 train_acc: 0.9967159277504105, train_prec: 0.9967229220590919, train_rec: 0.9967159277504105, train_f1: 0.9967159147392369\n",
      "Epoch 21 val_acc: 0.7148217636022514, val_prec: 0.7153341589835329, val_rec: 0.7148217636022514, val_f1: 0.7146520146520147\n",
      "Epoch 21 test_acc: 0.695423855963991, test_prec: 0.6963749364307368, test_rec: 0.695423855963991, test_f1: 0.6950686329056867\n",
      "Loss 9.123981142275527\n",
      "Epoch 22 train_acc: 0.9727891156462585, train_prec: 0.9740057367484809, train_rec: 0.9727891156462585, train_f1: 0.9727714777710917\n",
      "Epoch 22 val_acc: 0.701688555347092, val_prec: 0.7148669679185495, val_rec: 0.701688555347092, val_f1: 0.6970432519581171\n",
      "Epoch 22 test_acc: 0.673668417104276, test_prec: 0.6856630271123801, test_rec: 0.673668417104276, test_f1: 0.6683699715463359\n",
      "Loss 9.91791234480237\n",
      "Epoch 23 train_acc: 0.9967159277504105, train_prec: 0.9967176755847243, train_rec: 0.9967159277504105, train_f1: 0.9967159241362068\n",
      "Epoch 23 val_acc: 0.7101313320825516, val_prec: 0.7103453136011276, val_rec: 0.7101313320825516, val_f1: 0.7100575934341902\n",
      "Epoch 23 test_acc: 0.700675168792198, test_prec: 0.7010141232548396, test_rec: 0.700675168792198, test_f1: 0.7005568687732769\n",
      "Loss 9.489943937411795\n",
      "Epoch 24 train_acc: 0.9967159277504105, train_prec: 0.996716365791518, train_rec: 0.9967159277504105, train_f1: 0.9967159273889906\n",
      "Epoch 24 val_acc: 0.7129455909943715, val_prec: 0.7129575848461487, val_rec: 0.7129455909943715, val_f1: 0.7129415491842229\n",
      "Epoch 24 test_acc: 0.695423855963991, test_prec: 0.6955310569955813, test_rec: 0.695423855963991, test_f1: 0.6953772249208875\n",
      "Loss 10.390782109079765\n",
      "Epoch 25 train_acc: 0.9985925404644617, train_prec: 0.9985942969697986, train_rec: 0.9985925404644617, train_f1: 0.9985925395350959\n",
      "Epoch 25 val_acc: 0.7110694183864915, val_prec: 0.7111437414829448, val_rec: 0.7110694183864915, val_f1: 0.711043990086741\n",
      "Epoch 25 test_acc: 0.7014253563390848, test_prec: 0.7020026394527914, test_rec: 0.7014253563390848, test_f1: 0.7012013648180901\n",
      "Loss 11.302025278668214\n",
      "Epoch 26 train_acc: 0.9988271170537181, train_prec: 0.9988281055900722, train_rec: 0.9988271170537181, train_f1: 0.9988271166664824\n",
      "Epoch 26 val_acc: 0.7091932457786116, val_prec: 0.7093376729965163, val_rec: 0.7091932457786116, val_f1: 0.7091430784396564\n",
      "Epoch 26 test_acc: 0.7029257314328582, test_prec: 0.703574826488842, test_rec: 0.7029257314328582, test_f1: 0.7026777467524897\n",
      "Loss 11.273241739427974\n",
      "Epoch 27 train_acc: 0.9990616936429745, train_prec: 0.999062133233025, train_rec: 0.9990616936429745, train_f1: 0.9990616935397116\n",
      "Epoch 27 val_acc: 0.7054409005628518, val_prec: 0.7054871930287826, val_rec: 0.7054409005628518, val_f1: 0.7054243099484991\n",
      "Epoch 27 test_acc: 0.7014253563390848, test_prec: 0.701678811103563, test_rec: 0.7014253563390848, test_f1: 0.7013245015189699\n",
      "Loss 11.73830786977028\n",
      "Epoch 28 train_acc: 0.997654234107436, train_prec: 0.9976546717358774, train_rec: 0.997654234107436, train_f1: 0.9976542333329643\n",
      "Epoch 28 val_acc: 0.7120075046904315, val_prec: 0.712277251452095, val_rec: 0.7120075046904315, val_f1: 0.711915985545711\n",
      "Epoch 28 test_acc: 0.6961740435108777, test_prec: 0.6962785854026816, test_rec: 0.6961740435108777, test_f1: 0.6961381321571082\n",
      "Loss 11.639988786255666\n",
      "Epoch 29 train_acc: 0.9988271170537181, train_prec: 0.9988281055900722, train_rec: 0.9988271170537181, train_f1: 0.9988271166664824\n",
      "Epoch 29 val_acc: 0.7157598499061913, val_prec: 0.7158213853130897, val_rec: 0.7157598499061913, val_f1: 0.7157395876878396\n",
      "Epoch 29 test_acc: 0.6991747936984246, test_prec: 0.6993656117517753, test_rec: 0.6991747936984246, test_f1: 0.6990965563340221\n",
      "Saving as best model...\n",
      "Loss 11.10872180658626\n",
      "Epoch 30 train_acc: 0.9948393150363594, train_prec: 0.9948828966273884, train_rec: 0.9948393150363594, train_f1: 0.9948392071245669\n",
      "Epoch 30 val_acc: 0.6969981238273921, val_prec: 0.7029128959276019, val_rec: 0.6969981238273921, val_f1: 0.6947738403918179\n",
      "Epoch 30 test_acc: 0.6909227306826706, test_prec: 0.6974994664288975, test_rec: 0.6909227306826706, test_f1: 0.6882901283399135\n",
      "Loss 10.459678507524194\n",
      "Epoch 31 train_acc: 0.997654234107436, train_prec: 0.9976581789800245, train_rec: 0.997654234107436, train_f1: 0.9976542302350749\n",
      "Epoch 31 val_acc: 0.7035647279549718, val_prec: 0.7049614925235598, val_rec: 0.7035647279549718, val_f1: 0.7030588318259552\n",
      "Epoch 31 test_acc: 0.6879219804951238, test_prec: 0.6884592007583282, test_rec: 0.6879219804951238, test_f1: 0.6876878587043355\n",
      "Loss 11.487510668135274\n",
      "Epoch 32 train_acc: 0.9978888106966924, train_prec: 0.9978941821688515, train_rec: 0.9978888106966924, train_f1: 0.9978888058175143\n",
      "Epoch 32 val_acc: 0.7120075046904315, val_prec: 0.71381457499077, val_rec: 0.7120075046904315, val_f1: 0.7113977186043539\n",
      "Epoch 32 test_acc: 0.6856714178544636, test_prec: 0.6861724692689962, test_rec: 0.6856714178544636, test_f1: 0.685448363351825\n",
      "Loss 11.1510810075853\n",
      "Epoch 33 train_acc: 0.9988271170537181, train_prec: 0.9988272267188892, train_rec: 0.9988271170537181, train_f1: 0.9988271169246393\n",
      "Epoch 33 val_acc: 0.7129455909943715, val_prec: 0.7145435910530441, val_rec: 0.7129455909943715, val_f1: 0.7124100719424461\n",
      "Epoch 33 test_acc: 0.6961740435108777, test_prec: 0.6980799333114499, test_rec: 0.6961740435108777, test_f1: 0.6954610917003761\n",
      "Loss 12.174370509772322\n",
      "Epoch 34 train_acc: 0.9985925404644617, train_prec: 0.9985942969697986, train_rec: 0.9985925404644617, train_f1: 0.9985925395350959\n",
      "Epoch 34 val_acc: 0.7073170731707317, val_prec: 0.7078907890789079, val_rec: 0.7073170731707317, val_f1: 0.7071150045968023\n",
      "Epoch 34 test_acc: 0.695423855963991, test_prec: 0.6955603256045155, test_rec: 0.695423855963991, test_f1: 0.6953652219085411\n",
      "Loss 10.79588972069281\n",
      "Epoch 35 train_acc: 0.9741965751817968, train_prec: 0.975416835815752, train_rec: 0.9741965751817968, train_f1: 0.9741798496105181\n",
      "Epoch 35 val_acc: 0.6979362101313321, val_prec: 0.7126043049049299, val_rec: 0.6979362101313321, val_f1: 0.6926347384746534\n",
      "Epoch 35 test_acc: 0.6939234808702176, test_prec: 0.7111400087428547, test_rec: 0.6939234808702176, test_f1: 0.6876109321661577\n",
      "Loss 9.121836719409776\n",
      "Epoch 36 train_acc: 0.9974196575181797, train_prec: 0.9974285284519668, train_rec: 0.9974196575181797, train_f1: 0.9974196472951243\n",
      "Epoch 36 val_acc: 0.700750469043152, val_prec: 0.7045881762089252, val_rec: 0.700750469043152, val_f1: 0.6993405108808189\n",
      "Epoch 36 test_acc: 0.6946736684171043, test_prec: 0.6975643129210932, test_rec: 0.6946736684171043, test_f1: 0.6935280252175304\n",
      "Loss 11.702090292233436\n",
      "Epoch 37 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 37 val_acc: 0.7045028142589118, val_prec: 0.7045380932263062, val_rec: 0.7045028142589118, val_f1: 0.7044900717851778\n",
      "Epoch 37 test_acc: 0.695423855963991, test_prec: 0.695423855963991, test_rec: 0.695423855963991, test_f1: 0.695423855963991\n",
      "Loss 11.884150750208757\n",
      "Epoch 38 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 38 val_acc: 0.702626641651032, val_prec: 0.7030734061468124, val_rec: 0.702626641651032, val_f1: 0.702462995198687\n",
      "Epoch 38 test_acc: 0.6946736684171043, test_prec: 0.6948978691018011, test_rec: 0.6946736684171043, test_f1: 0.6945787864143076\n",
      "Loss 11.873309765221276\n",
      "Epoch 39 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 39 val_acc: 0.698874296435272, val_prec: 0.6996395759717314, val_rec: 0.698874296435272, val_f1: 0.6985854428958124\n",
      "Epoch 39 test_acc: 0.6969242310577645, test_prec: 0.6973683111548389, test_rec: 0.6969242310577645, test_f1: 0.6967440035479081\n",
      "Loss 11.930458072730485\n",
      "Epoch 40 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 40 val_acc: 0.7054409005628518, val_prec: 0.705626195685491, val_rec: 0.7054409005628518, val_f1: 0.7053745268902386\n",
      "Epoch 40 test_acc: 0.6976744186046512, test_prec: 0.6977245341531965, test_rec: 0.6976744186046512, test_f1: 0.6976519578819564\n",
      "Loss 12.06175632822777\n",
      "Epoch 41 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 41 val_acc: 0.7073170731707317, val_prec: 0.7073170731707317, val_rec: 0.7073170731707317, val_f1: 0.7073170731707317\n",
      "Epoch 41 test_acc: 0.7021755438859715, test_prec: 0.702176334229025, test_rec: 0.7021755438859715, test_f1: 0.7021755438859715\n",
      "Loss 12.11761965688805\n",
      "Epoch 42 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 42 val_acc: 0.7045028142589118, val_prec: 0.7048843278318522, val_rec: 0.7045028142589118, val_f1: 0.7043651894170504\n",
      "Epoch 42 test_acc: 0.6984246061515379, test_prec: 0.6986335690468245, test_rec: 0.6984246061515379, test_f1: 0.6983387017463006\n",
      "Loss 12.060869605768392\n",
      "Epoch 43 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 43 val_acc: 0.7073170731707317, val_prec: 0.7073433478963432, val_rec: 0.7073170731707317, val_f1: 0.7073078006195438\n",
      "Epoch 43 test_acc: 0.7036759189797449, test_prec: 0.7036960533118335, test_rec: 0.7036759189797449, test_f1: 0.7036665797659579\n",
      "Loss 12.096005776097764\n",
      "Epoch 44 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 44 val_acc: 0.702626641651032, val_prec: 0.7031479390175042, val_rec: 0.702626641651032, val_f1: 0.7024357469486019\n",
      "Epoch 44 test_acc: 0.6969242310577645, test_prec: 0.6972154936732019, test_rec: 0.6969242310577645, test_f1: 0.6968044450081983\n",
      "Loss 12.042217051381458\n",
      "Epoch 45 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 45 val_acc: 0.7045028142589118, val_prec: 0.7047630124626403, val_rec: 0.7045028142589118, val_f1: 0.7044089102504854\n",
      "Epoch 45 test_acc: 0.7029257314328582, test_prec: 0.7030106253624837, test_rec: 0.7029257314328582, test_f1: 0.7028906175360274\n",
      "Loss 12.053472800099366\n",
      "Epoch 46 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 46 val_acc: 0.702626641651032, val_prec: 0.7030734061468124, val_rec: 0.702626641651032, val_f1: 0.702462995198687\n",
      "Epoch 46 test_acc: 0.700675168792198, test_prec: 0.7009066807915043, test_rec: 0.700675168792198, test_f1: 0.7005821517919133\n",
      "Loss 12.02528227210485\n",
      "Epoch 47 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 47 val_acc: 0.702626641651032, val_prec: 0.7030734061468124, val_rec: 0.702626641651032, val_f1: 0.702462995198687\n",
      "Epoch 47 test_acc: 0.7014253563390848, test_prec: 0.7016376668065637, test_rec: 0.7014253563390848, test_f1: 0.7013403067040488\n",
      "Loss 12.008722376683817\n",
      "Epoch 48 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 48 val_acc: 0.701688555347092, val_prec: 0.7021696252465484, val_rec: 0.701688555347092, val_f1: 0.701510989010989\n",
      "Epoch 48 test_acc: 0.7014253563390848, test_prec: 0.7016376668065637, test_rec: 0.7014253563390848, test_f1: 0.7013403067040488\n",
      "Loss 11.984909830271262\n",
      "Epoch 49 train_acc: 0.9992962702322308, train_prec: 0.9992963802071423, train_rec: 0.9992962702322308, train_f1: 0.9992962702322308\n",
      "Epoch 49 val_acc: 0.7045028142589118, val_prec: 0.7051969427255517, val_rec: 0.7045028142589118, val_f1: 0.7042527050057469\n",
      "Epoch 49 test_acc: 0.7029257314328582, test_prec: 0.7032751478503576, test_rec: 0.7029257314328582, test_f1: 0.7027899106070267\n",
      "Loss 11.967292097557559\n"
     ]
    }
   ],
   "source": [
    "best_mlp = None\n",
    "best_f1 = 0.0\n",
    "losses = []\n",
    "batch_size = 16\n",
    "n_batch = x_train.shape[0] // batch_size\n",
    "if x_train.shape[0] % batch_size != 0:\n",
    "    n_batch+=1\n",
    "best_epno = -1\n",
    "for epoch in range(50):\n",
    "    mymlp.train_mode()\n",
    "    for i in range(n_batch):\n",
    "        _, loss = mymlp.forward(x_train[i*batch_size:(i+1)*batch_size ], \n",
    "                                y_train[i*batch_size:(i+1)*batch_size ])\n",
    "        mymlp.backward()\n",
    "        _ = mymlp.optimize(1)\n",
    "\n",
    "    train_acc, train_prec, train_rec, train_f1 = tester(\n",
    "        mymlp, x_train, y_train)\n",
    "    print(\"Epoch {} train_acc: {}, train_prec: {}, train_rec: {}, train_f1: {}\".format(\n",
    "        epoch, train_acc, train_prec, train_rec, train_f1))\n",
    "    if epoch % 1 == 0:\n",
    "        val_acc, val_prec, val_rec, val_f1 = tester(\n",
    "            mymlp, x_val, y_val)\n",
    "        print(\"Epoch {} val_acc: {}, val_prec: {}, val_rec: {}, val_f1: {}\".format(\n",
    "            epoch, val_acc, val_prec, val_rec, val_f1))\n",
    "\n",
    "        test_acc, test_prec, test_rec, test_f1 = tester(\n",
    "            mymlp, x_test, y_test)\n",
    "        print(\"Epoch {} test_acc: {}, test_prec: {}, test_rec: {}, test_f1: {}\".format(\n",
    "            epoch, test_acc, test_prec, test_rec, test_f1))\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_mlp = deepcopy(mymlp)\n",
    "            best_f1 = val_f1\n",
    "            print(\"Saving as best model...\")\n",
    "            best_epno = epoch\n",
    "#         if best_epno + 30 < epoch:\n",
    "#             print('Validation not improving')\n",
    "#             break\n",
    "            \n",
    "    \n",
    "    print('Loss', loss)\n",
    "    losses.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7157395876878396"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 test_acc: 0.7029257314328582, test_prec: 0.7032751478503576, test_rec: 0.7029257314328582, test_f1: 0.7027899106070267\n"
     ]
    }
   ],
   "source": [
    "test_acc, test_prec, test_rec, test_f1 = tester(\n",
    "            mymlp, x_test, y_test)\n",
    "print(\"Epoch {} test_acc: {}, test_prec: {}, test_rec: {}, test_f1: {}\".format(\n",
    "            epoch, test_acc, test_prec, test_rec, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train_acc: 0.6875439831104856, train_prec: 0.7377864619684348, train_rec: 0.6875439831104856, train_f1: 0.6700904756681132\n",
      "Epoch 0 val_acc: 0.6050656660412758, val_prec: 0.6455091455091455, val_rec: 0.6050656660412758, val_f1: 0.5755739340549181\n",
      "Epoch 0 test_acc: 0.582145536384096, test_prec: 0.6186598590120247, test_rec: 0.582145536384096, test_f1: 0.5475780656653595\n",
      "Saving as best model...\n",
      "Loss 1.2391771322058707\n",
      "Epoch 1 train_acc: 0.8501055594651654, train_prec: 0.8541902325306051, train_rec: 0.8501055594651654, train_f1: 0.8496698628326832\n",
      "Epoch 1 val_acc: 0.6960600375234521, val_prec: 0.7026335798714315, val_rec: 0.6960600375234521, val_f1: 0.6935748886562451\n",
      "Epoch 1 test_acc: 0.6669167291822956, test_prec: 0.6716318173826157, test_rec: 0.6669167291822956, test_f1: 0.6646540377853704\n",
      "Saving as best model...\n",
      "Loss 1.9602254107489698\n",
      "Epoch 2 train_acc: 0.8937368050668544, train_prec: 0.8987475679563564, train_rec: 0.8937368050668544, train_f1: 0.8934003399598865\n",
      "Epoch 2 val_acc: 0.6923076923076923, val_prec: 0.7022190143060721, val_rec: 0.6923076923076923, val_f1: 0.6884907058957691\n",
      "Epoch 2 test_acc: 0.6564141035258815, test_prec: 0.6657887219625687, test_rec: 0.6564141035258815, test_f1: 0.651550048517471\n",
      "Loss 4.600577152073422\n",
      "Epoch 3 train_acc: 0.9221205723668778, train_prec: 0.9245138594880147, train_rec: 0.9221205723668778, train_f1: 0.9220113997381406\n",
      "Epoch 3 val_acc: 0.6894934333958724, val_prec: 0.6927232500859205, val_rec: 0.6894934333958724, val_f1: 0.6881870280444858\n",
      "Epoch 3 test_acc: 0.6751687921980495, test_prec: 0.6792490209803558, test_rec: 0.6751687921980495, test_f1: 0.6732744024337572\n",
      "Loss 5.524254992125476\n",
      "Epoch 4 train_acc: 0.9427633122214403, train_prec: 0.9434857883324046, train_rec: 0.9427633122214403, train_f1: 0.9427402801022363\n",
      "Epoch 4 val_acc: 0.7063789868667918, val_prec: 0.707272752983766, val_rec: 0.7063789868667918, val_f1: 0.7060621197024836\n",
      "Epoch 4 test_acc: 0.6834208552138035, test_prec: 0.6849998480597448, test_rec: 0.6834208552138035, test_f1: 0.6827233854241147\n",
      "Saving as best model...\n",
      "Loss 6.366253594462158\n",
      "Epoch 5 train_acc: 0.9568379075768239, train_prec: 0.9568523305846617, train_rec: 0.9568379075768239, train_f1: 0.9568375370663732\n",
      "Epoch 5 val_acc: 0.7148217636022514, val_prec: 0.7148974079981971, val_rec: 0.7148217636022514, val_f1: 0.71479666554016\n",
      "Epoch 5 test_acc: 0.6856714178544636, test_prec: 0.6857258974649272, test_rec: 0.6856714178544636, test_f1: 0.6856519579500877\n",
      "Saving as best model...\n",
      "Loss 7.612779434520233\n",
      "Epoch 6 train_acc: 0.9514426460239268, train_prec: 0.9517437148317022, train_rec: 0.9514426460239268, train_f1: 0.9514347091316168\n",
      "Epoch 6 val_acc: 0.7035647279549718, val_prec: 0.704211674888502, val_rec: 0.7035647279549718, val_f1: 0.7033297635487417\n",
      "Epoch 6 test_acc: 0.6879219804951238, test_prec: 0.6887291205628004, test_rec: 0.6879219804951238, test_f1: 0.6875738329871346\n",
      "Loss 7.273599694200499\n",
      "Epoch 7 train_acc: 0.9549612948627727, train_prec: 0.9573493423897845, train_rec: 0.9549612948627727, train_f1: 0.9549028256615026\n",
      "Epoch 7 val_acc: 0.6904315196998124, val_prec: 0.6968163479134, val_rec: 0.6904315196998124, val_f1: 0.6879003495572867\n",
      "Epoch 7 test_acc: 0.6556639159789948, test_prec: 0.660395735744014, test_rec: 0.6556639159789948, test_f1: 0.6530590858772329\n",
      "Loss 6.846280855686919\n",
      "Epoch 8 train_acc: 0.9784189537884119, train_prec: 0.9785555489121234, train_rec: 0.9784189537884119, train_f1: 0.9784174574092558\n",
      "Epoch 8 val_acc: 0.7120075046904315, val_prec: 0.7135295535764933, val_rec: 0.7120075046904315, val_f1: 0.7114933824430436\n",
      "Epoch 8 test_acc: 0.6879219804951238, test_prec: 0.6890546317225155, test_rec: 0.6879219804951238, test_f1: 0.6874371731461004\n",
      "Loss 7.79168403205636\n",
      "Epoch 9 train_acc: 0.9765423410743608, train_prec: 0.9766366832342511, train_rec: 0.9765423410743608, train_f1: 0.9765411405805736\n",
      "Epoch 9 val_acc: 0.7110694183864915, val_prec: 0.7111764602299742, val_rec: 0.7110694183864915, val_f1: 0.7110328002168609\n",
      "Epoch 9 test_acc: 0.6909227306826706, test_prec: 0.6912227536111751, test_rec: 0.6909227306826706, test_f1: 0.6908096279991416\n",
      "Loss 6.147224435823588\n",
      "Epoch 10 train_acc: 0.9547267182735163, train_prec: 0.9563989946919552, train_rec: 0.9547267182735163, train_f1: 0.9546855462032479\n",
      "Epoch 10 val_acc: 0.6810506566604128, val_prec: 0.6873854673297266, val_rec: 0.6810506566604128, val_f1: 0.6783320553236204\n",
      "Epoch 10 test_acc: 0.6714178544636159, test_prec: 0.6802541564400741, test_rec: 0.6714178544636159, test_f1: 0.6672884061096246\n",
      "Loss 7.79342188960542\n",
      "Epoch 11 train_acc: 0.9861599812338728, train_prec: 0.9862269069661272, train_rec: 0.9861599812338728, train_f1: 0.9861595242814029\n",
      "Epoch 11 val_acc: 0.698874296435272, val_prec: 0.6991273332205493, val_rec: 0.698874296435272, val_f1: 0.698778603779066\n",
      "Epoch 11 test_acc: 0.6834208552138035, test_prec: 0.6838332462578419, test_rec: 0.6834208552138035, test_f1: 0.683232597765389\n",
      "Loss 8.786975881057879\n",
      "Epoch 12 train_acc: 0.9634060520760028, train_prec: 0.9648301966531798, train_rec: 0.9634060520760028, train_f1: 0.9633777547033296\n",
      "Epoch 12 val_acc: 0.701688555347092, val_prec: 0.7099940260873071, val_rec: 0.701688555347092, val_f1: 0.6987094709897611\n",
      "Epoch 12 test_acc: 0.6714178544636159, test_prec: 0.6782450995809506, test_rec: 0.6714178544636159, test_f1: 0.6682874304661762\n",
      "Loss 7.546162917934063\n",
      "Epoch 13 train_acc: 0.9960121979826413, train_prec: 0.9960367699919209, train_rec: 0.9960121979826413, train_f1: 0.9960121519011103\n",
      "Epoch 13 val_acc: 0.7063789868667918, val_prec: 0.7086001764722626, val_rec: 0.7063789868667918, val_f1: 0.7055952762255687\n",
      "Epoch 13 test_acc: 0.6909227306826706, test_prec: 0.6920739383742339, test_rec: 0.6909227306826706, test_f1: 0.6904425849427724\n",
      "Loss 11.159000794936167\n",
      "Epoch 14 train_acc: 0.9779498006098991, train_prec: 0.9781018472411596, train_rec: 0.9779498006098991, train_f1: 0.977948094527929\n",
      "Epoch 14 val_acc: 0.7138836772983115, val_prec: 0.7145187256397221, val_rec: 0.7138836772983115, val_f1: 0.7136717700778943\n",
      "Epoch 14 test_acc: 0.6901725431357839, test_prec: 0.6905231508033994, test_rec: 0.6901725431357839, test_f1: 0.6900207686104509\n",
      "Loss 9.748728944627826\n",
      "Epoch 15 train_acc: 0.9894440534834623, train_prec: 0.9894450195613975, train_rec: 0.9894440534834623, train_f1: 0.9894440465132157\n",
      "Epoch 15 val_acc: 0.7082551594746717, val_prec: 0.7083438979039891, val_rec: 0.7082551594746717, val_f1: 0.7082240909670975\n",
      "Epoch 15 test_acc: 0.6924231057764441, test_prec: 0.6925031979597018, test_rec: 0.6924231057764441, test_f1: 0.6923867504792203\n",
      "Loss 10.222250700029436\n",
      "Epoch 16 train_acc: 0.9917898193760263, train_prec: 0.9918288820142809, train_rec: 0.9917898193760263, train_f1: 0.9917896476978061\n",
      "Epoch 16 val_acc: 0.7185741088180112, val_prec: 0.7200738606358961, val_rec: 0.7185741088180112, val_f1: 0.7180938276828688\n",
      "Epoch 16 test_acc: 0.6946736684171043, test_prec: 0.695662695477516, test_rec: 0.6946736684171043, test_f1: 0.6943017247776179\n",
      "Saving as best model...\n",
      "Loss 11.674973317787249\n",
      "Epoch 17 train_acc: 0.9964813511611541, train_prec: 0.996499824792488, train_rec: 0.9964813511611541, train_f1: 0.9964813209565394\n",
      "Epoch 17 val_acc: 0.7157598499061913, val_prec: 0.7158517861167456, val_rec: 0.7157598499061913, val_f1: 0.7157295805885227\n",
      "Epoch 17 test_acc: 0.6894223555888972, test_prec: 0.6894305767297502, test_rec: 0.6894223555888972, test_f1: 0.689420258130581\n",
      "Loss 11.266874164634197\n",
      "Epoch 18 train_acc: 0.9847525216983345, train_prec: 0.9851764695745863, train_rec: 0.9847525216983345, train_f1: 0.9847492438302659\n",
      "Epoch 18 val_acc: 0.7120075046904315, val_prec: 0.7192696956458423, val_rec: 0.7120075046904315, val_f1: 0.7096030248095074\n",
      "Epoch 18 test_acc: 0.6691672918229558, test_prec: 0.6761335203641435, test_rec: 0.6691672918229558, test_f1: 0.6658152025022532\n",
      "Loss 11.311167555016938\n",
      "Epoch 19 train_acc: 0.9964813511611541, train_prec: 0.9964823358251471, train_rec: 0.9964813511611541, train_f1: 0.9964813499994472\n",
      "Epoch 19 val_acc: 0.7213883677298312, val_prec: 0.7229777639117363, val_rec: 0.7213883677298312, val_f1: 0.7208909921354526\n",
      "Epoch 19 test_acc: 0.6901725431357839, test_prec: 0.6919089197121454, test_rec: 0.6901725431357839, test_f1: 0.6894899964172789\n",
      "Saving as best model...\n",
      "Loss 12.667326323952699\n",
      "Epoch 20 train_acc: 0.9833450621627962, train_prec: 0.9837677891973039, train_rec: 0.9833450621627962, train_f1: 0.9833414817222904\n",
      "Epoch 20 val_acc: 0.702626641651032, val_prec: 0.7089892535579436, val_rec: 0.702626641651032, val_f1: 0.7003459231874946\n",
      "Epoch 20 test_acc: 0.6796699174793699, test_prec: 0.6835229577276266, test_rec: 0.6796699174793699, test_f1: 0.6779469029319476\n",
      "Loss 10.29250049532059\n",
      "Epoch 21 train_acc: 0.9943701618578465, train_prec: 0.9944228464369655, train_rec: 0.9943701618578465, train_f1: 0.9943700187321021\n",
      "Epoch 21 val_acc: 0.7045028142589118, val_prec: 0.7067038112316056, val_rec: 0.7045028142589118, val_f1: 0.7037140958819622\n",
      "Epoch 21 test_acc: 0.668417104276069, test_prec: 0.6698640566445265, test_rec: 0.668417104276069, test_f1: 0.6676865790461106\n",
      "Loss 11.577013298983983\n",
      "Epoch 22 train_acc: 0.9962467745718977, train_prec: 0.9962681895586346, train_rec: 0.9962467745718977, train_f1: 0.9962467369838808\n",
      "Epoch 22 val_acc: 0.7148217636022514, val_prec: 0.7159192626793328, val_rec: 0.7148217636022514, val_f1: 0.7144589184007218\n",
      "Epoch 22 test_acc: 0.668417104276069, test_prec: 0.6692772554941981, test_rec: 0.668417104276069, test_f1: 0.6679776068149745\n",
      "Loss 12.742311032056548\n",
      "Epoch 23 train_acc: 0.9974196575181797, train_prec: 0.9974223960604229, train_rec: 0.9974196575181797, train_f1: 0.9974196546784496\n",
      "Epoch 23 val_acc: 0.7129455909943715, val_prec: 0.713137654888614, val_rec: 0.7129455909943715, val_f1: 0.7128809083707419\n",
      "Epoch 23 test_acc: 0.6894223555888972, test_prec: 0.6899891802159366, test_rec: 0.6894223555888972, test_f1: 0.6892019716469139\n",
      "Loss 12.408206305716915\n",
      "Epoch 24 train_acc: 0.9983579638752053, train_prec: 0.9983607070642961, train_rec: 0.9983579638752053, train_f1: 0.9983579620681043\n",
      "Epoch 24 val_acc: 0.7101313320825516, val_prec: 0.710137989298789, val_rec: 0.7101313320825516, val_f1: 0.7101290362890913\n",
      "Epoch 24 test_acc: 0.6856714178544636, test_prec: 0.6856836366353374, test_rec: 0.6856714178544636, test_f1: 0.685667879857295\n",
      "Loss 12.178130421211433\n",
      "Epoch 25 train_acc: 0.9964813511611541, train_prec: 0.996499824792488, train_rec: 0.9964813511611541, train_f1: 0.9964813209565394\n",
      "Epoch 25 val_acc: 0.702626641651032, val_prec: 0.7030046550994499, val_rec: 0.702626641651032, val_f1: 0.7024881430006505\n",
      "Epoch 25 test_acc: 0.6826706676669168, test_prec: 0.6827578805048978, test_rec: 0.6826706676669168, test_f1: 0.682627800603342\n",
      "Loss 12.42611265958694\n",
      "Epoch 26 train_acc: 0.9974196575181797, train_prec: 0.9974329085766764, train_rec: 0.9974196575181797, train_f1: 0.9974196418995921\n",
      "Epoch 26 val_acc: 0.7063789868667918, val_prec: 0.7069917528102583, val_rec: 0.7063789868667918, val_f1: 0.7061615214241994\n",
      "Epoch 26 test_acc: 0.6811702925731433, test_prec: 0.6818437643309132, test_rec: 0.6811702925731433, test_f1: 0.6808610046321875\n",
      "Loss 12.967906974819483\n",
      "Epoch 27 train_acc: 0.9983579638752053, train_prec: 0.998358951637087, train_rec: 0.9983579638752053, train_f1: 0.9983579633330751\n",
      "Epoch 27 val_acc: 0.7129455909943715, val_prec: 0.7129455909943715, val_rec: 0.7129455909943715, val_f1: 0.7129455909943714\n",
      "Epoch 27 test_acc: 0.6909227306826706, test_prec: 0.6909402916700647, test_rec: 0.6909227306826706, test_f1: 0.6909175123215133\n",
      "Loss 12.64950787147723\n",
      "Epoch 28 train_acc: 0.9981233872859488, train_prec: 0.998130405940326, train_rec: 0.9981233872859488, train_f1: 0.9981233815032153\n",
      "Epoch 28 val_acc: 0.7073170731707317, val_prec: 0.7080670517372335, val_rec: 0.7073170731707317, val_f1: 0.7070530910782045\n",
      "Epoch 28 test_acc: 0.687171792948237, test_prec: 0.6880130969479057, test_rec: 0.687171792948237, test_f1: 0.6868069269126386\n",
      "Loss 12.293203982155584\n",
      "Epoch 29 train_acc: 0.9978888106966924, train_prec: 0.9978976895302111, train_rec: 0.9978888106966924, train_f1: 0.9978888023323745\n",
      "Epoch 29 val_acc: 0.7101313320825516, val_prec: 0.7111488398415393, val_rec: 0.7101313320825516, val_f1: 0.7097816979401526\n",
      "Epoch 29 test_acc: 0.6849212303075769, test_prec: 0.6855108308856588, test_rec: 0.6849212303075769, test_f1: 0.6846582162526407\n",
      "Loss 12.192623527187548\n",
      "Epoch 30 train_acc: 0.9978888106966924, train_prec: 0.9978976895302111, train_rec: 0.9978888106966924, train_f1: 0.9978888023323745\n",
      "Epoch 30 val_acc: 0.7110694183864915, val_prec: 0.7118329712256902, val_rec: 0.7110694183864915, val_f1: 0.7108088206797659\n",
      "Epoch 30 test_acc: 0.6841710427606902, test_prec: 0.6846678856515706, test_rec: 0.6841710427606902, test_f1: 0.6839469235587552\n",
      "Loss 12.211382439510732\n",
      "Epoch 31 train_acc: 0.9983579638752053, train_prec: 0.9983633400458664, train_rec: 0.9983579638752053, train_f1: 0.998357960080289\n",
      "Epoch 31 val_acc: 0.7101313320825516, val_prec: 0.7108445650024018, val_rec: 0.7101313320825516, val_f1: 0.7098859868151614\n",
      "Epoch 31 test_acc: 0.6849212303075769, test_prec: 0.6853371423575083, test_rec: 0.6849212303075769, test_f1: 0.6847338650745578\n",
      "Loss 12.355047730703815\n",
      "Epoch 32 train_acc: 0.9983579638752053, train_prec: 0.9983633400458664, train_rec: 0.9983579638752053, train_f1: 0.998357960080289\n",
      "Epoch 32 val_acc: 0.7101313320825516, val_prec: 0.7108445650024018, val_rec: 0.7101313320825516, val_f1: 0.7098859868151614\n",
      "Epoch 32 test_acc: 0.687171792948237, test_prec: 0.687516610314274, test_rec: 0.687171792948237, test_f1: 0.6870185484517142\n",
      "Loss 12.494270750882071\n",
      "Epoch 33 train_acc: 0.9988271170537181, train_prec: 0.9988298625662327, train_rec: 0.9988271170537181, train_f1: 0.9988271157629315\n",
      "Epoch 33 val_acc: 0.7101313320825516, val_prec: 0.7107552392249901, val_rec: 0.7101313320825516, val_f1: 0.709916645751047\n",
      "Epoch 33 test_acc: 0.6879219804951238, test_prec: 0.6882443948304683, test_rec: 0.6879219804951238, test_f1: 0.6877793000316239\n",
      "Loss 12.61974681473636\n",
      "Epoch 34 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 34 val_acc: 0.7101313320825516, val_prec: 0.7107552392249901, val_rec: 0.7101313320825516, val_f1: 0.709916645751047\n",
      "Epoch 34 test_acc: 0.6886721680420105, test_prec: 0.6889292368090355, test_rec: 0.6886721680420105, test_f1: 0.6885582385241134\n",
      "Loss 12.731247399324769\n",
      "Epoch 35 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 35 val_acc: 0.7120075046904315, val_prec: 0.7124749527276832, val_rec: 0.7120075046904315, val_f1: 0.711849020586741\n",
      "Epoch 35 test_acc: 0.6886721680420105, test_prec: 0.6889292368090355, test_rec: 0.6886721680420105, test_f1: 0.6885582385241134\n",
      "Loss 12.83131334614863\n",
      "Epoch 36 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 36 val_acc: 0.7110694183864915, val_prec: 0.7114296292378485, val_rec: 0.7110694183864915, val_f1: 0.7109463038088799\n",
      "Epoch 36 test_acc: 0.687171792948237, test_prec: 0.6873868544896722, test_rec: 0.687171792948237, test_f1: 0.6870745796923005\n",
      "Loss 12.922227355279587\n",
      "Epoch 37 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 37 val_acc: 0.7110694183864915, val_prec: 0.7113670251578312, val_rec: 0.7110694183864915, val_f1: 0.710967678325569\n",
      "Epoch 37 test_acc: 0.687171792948237, test_prec: 0.6873868544896722, test_rec: 0.687171792948237, test_f1: 0.6870745796923005\n",
      "Loss 13.005701221193288\n",
      "Epoch 38 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 38 val_acc: 0.7101313320825516, val_prec: 0.7103986917047312, val_rec: 0.7101313320825516, val_f1: 0.7100392167219045\n",
      "Epoch 38 test_acc: 0.6879219804951238, test_prec: 0.6881192268877377, test_rec: 0.6879219804951238, test_f1: 0.6878330843941817\n",
      "Loss 13.083104847141538\n",
      "Epoch 39 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 39 val_acc: 0.7138836772983115, val_prec: 0.714053208578756, val_rec: 0.7138836772983115, val_f1: 0.7138270146664424\n",
      "Epoch 39 test_acc: 0.687171792948237, test_prec: 0.6873503986461732, test_rec: 0.687171792948237, test_f1: 0.6870904338934843\n",
      "Loss 13.15549114459816\n",
      "Epoch 40 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 40 val_acc: 0.7148217636022514, val_prec: 0.71497007675427, val_rec: 0.7148217636022514, val_f1: 0.7147725672440501\n",
      "Epoch 40 test_acc: 0.6856714178544636, test_prec: 0.685815706273124, test_rec: 0.6856714178544636, test_f1: 0.6856041813777868\n",
      "Loss 13.223643908765121\n",
      "Epoch 41 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 41 val_acc: 0.7148217636022514, val_prec: 0.71497007675427, val_rec: 0.7148217636022514, val_f1: 0.7147725672440501\n",
      "Epoch 41 test_acc: 0.6849212303075769, test_prec: 0.6850498208493728, test_rec: 0.6849212303075769, test_f1: 0.684860574388146\n",
      "Loss 13.288153171067291\n",
      "Epoch 42 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 42 val_acc: 0.7157598499061913, val_prec: 0.7158882783882784, val_rec: 0.7157598499061913, val_f1: 0.7157175711392578\n",
      "Epoch 42 test_acc: 0.6849212303075769, test_prec: 0.6850221858313998, test_rec: 0.6849212303075769, test_f1: 0.6848729912974697\n",
      "Loss 13.349475461835452\n",
      "Epoch 43 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 43 val_acc: 0.7157598499061913, val_prec: 0.7158882783882784, val_rec: 0.7157598499061913, val_f1: 0.7157175711392578\n",
      "Epoch 43 test_acc: 0.6849212303075769, test_prec: 0.6850221858313998, test_rec: 0.6849212303075769, test_f1: 0.6848729912974697\n",
      "Loss 13.407973726529358\n",
      "Epoch 44 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 44 val_acc: 0.7157598499061913, val_prec: 0.7158517861167456, val_rec: 0.7157598499061913, val_f1: 0.7157295805885227\n",
      "Epoch 44 test_acc: 0.6849212303075769, test_prec: 0.6849978926720005, test_rec: 0.6849212303075769, test_f1: 0.6848839882957866\n",
      "Loss 13.463938133627755\n",
      "Epoch 45 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 45 val_acc: 0.7166979362101313, val_prec: 0.7167742412558232, val_rec: 0.7166979362101313, val_f1: 0.7166730032668693\n",
      "Epoch 45 test_acc: 0.6856714178544636, test_prec: 0.685737482474393, test_rec: 0.6856714178544636, test_f1: 0.6856392188184172\n",
      "Loss 13.517614452983098\n",
      "Epoch 46 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 46 val_acc: 0.7185741088180112, val_prec: 0.7186233606196638, val_rec: 0.7185741088180112, val_f1: 0.7185582579125789\n",
      "Epoch 46 test_acc: 0.6849212303075769, test_prec: 0.6849978926720005, test_rec: 0.6849212303075769, test_f1: 0.6848839882957866\n",
      "Loss 13.569221387753853\n",
      "Epoch 47 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 47 val_acc: 0.7185741088180112, val_prec: 0.7186233606196638, val_rec: 0.7185741088180112, val_f1: 0.7185582579125789\n",
      "Epoch 47 test_acc: 0.6856714178544636, test_prec: 0.685737482474393, test_rec: 0.6856714178544636, test_f1: 0.6856392188184172\n",
      "Loss 13.61895834721732\n",
      "Epoch 48 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 48 val_acc: 0.7176360225140713, val_prec: 0.7176735671032248, val_rec: 0.7176360225140713, val_f1: 0.7176238463725032\n",
      "Epoch 48 test_acc: 0.6849212303075769, test_prec: 0.6849978926720005, test_rec: 0.6849212303075769, test_f1: 0.6848839882957866\n",
      "Loss 13.667019201847536\n",
      "Epoch 49 train_acc: 0.9997654234107436, train_prec: 0.9997655334372865, train_rec: 0.9997654234107436, train_f1: 0.9997654234107436\n",
      "Epoch 49 val_acc: 0.7176360225140713, val_prec: 0.7176551762982989, val_rec: 0.7176360225140713, val_f1: 0.7176298103281527\n",
      "Epoch 49 test_acc: 0.6856714178544636, test_prec: 0.685737482474393, test_rec: 0.6856714178544636, test_f1: 0.6856392188184172\n",
      "Loss 13.713584827490195\n",
      "Epoch 0 train_acc: 0.7243725076237392, train_prec: 0.7431789271085677, train_rec: 0.7243725076237392, train_f1: 0.7189243726663509\n",
      "Epoch 0 val_acc: 0.649155722326454, val_prec: 0.6693361786814689, val_rec: 0.649155722326454, val_f1: 0.6383818594104308\n",
      "Epoch 0 test_acc: 0.6136534133533383, test_prec: 0.627227703490614, test_rec: 0.6136534133533383, test_f1: 0.6031872869938814\n",
      "Saving as best model...\n",
      "Loss 1.355855516759964\n",
      "Epoch 1 train_acc: 0.8221909453436547, train_prec: 0.8222722799281462, train_rec: 0.8221909453436547, train_f1: 0.822179301417444\n",
      "Epoch 1 val_acc: 0.6801125703564728, val_prec: 0.6803417357469126, val_rec: 0.6801125703564728, val_f1: 0.6800109155410015\n",
      "Epoch 1 test_acc: 0.6714178544636159, test_prec: 0.67211679530052, test_rec: 0.6714178544636159, test_f1: 0.6710991208534622\n",
      "Saving as best model...\n",
      "Loss 3.70089990202073\n",
      "Epoch 2 train_acc: 0.8716866056767535, train_prec: 0.8716885803332131, train_rec: 0.8716866056767535, train_f1: 0.8716863938584705\n",
      "Epoch 2 val_acc: 0.6941838649155723, val_prec: 0.6943179296425062, val_rec: 0.6941838649155723, val_f1: 0.6941311082946063\n",
      "Epoch 2 test_acc: 0.687171792948237, test_prec: 0.6872476046887308, test_rec: 0.6871717929482369, test_f1: 0.6871443263166719\n",
      "Saving as best model...\n",
      "Loss 4.538438578907847\n",
      "Epoch 3 train_acc: 0.8806005160684963, train_prec: 0.883555013962068, train_rec: 0.8806005160684963, train_f1: 0.880371559091004\n",
      "Epoch 3 val_acc: 0.6932457786116323, val_prec: 0.6954814129041448, val_rec: 0.6932457786116323, val_f1: 0.6923662107815061\n",
      "Epoch 3 test_acc: 0.6924231057764441, test_prec: 0.6961289736434743, test_rec: 0.6924231057764441, test_f1: 0.690934830481814\n",
      "Loss 4.394547928105835\n",
      "Epoch 4 train_acc: 0.9061693642974431, train_prec: 0.9061725207614738, train_rec: 0.9061693642974431, train_f1: 0.9061691474449323\n",
      "Epoch 4 val_acc: 0.702626641651032, val_prec: 0.7026444744846232, val_rec: 0.702626641651032, val_f1: 0.7026200992492504\n",
      "Epoch 4 test_acc: 0.6744186046511628, test_prec: 0.6744542126813182, test_rec: 0.6744186046511628, test_f1: 0.6743984478532644\n",
      "Saving as best model...\n",
      "Loss 5.465786716996068\n",
      "Epoch 5 train_acc: 0.9115646258503401, train_prec: 0.9115754801731398, train_rec: 0.9115646258503401, train_f1: 0.9115639834988666\n",
      "Epoch 5 val_acc: 0.699812382739212, val_prec: 0.6998574069184051, val_rec: 0.699812382739212, val_f1: 0.6997954751067509\n",
      "Epoch 5 test_acc: 0.7021755438859715, test_prec: 0.7027463310283051, test_rec: 0.7021755438859715, test_f1: 0.7019759580795605\n",
      "Loss 5.4246332304569975\n",
      "Epoch 6 train_acc: 0.9230588787239034, train_prec: 0.9240692979047196, train_rec: 0.9230588787239034, train_f1: 0.9230135000277776\n",
      "Epoch 6 val_acc: 0.7213883677298312, val_prec: 0.7227061556329849, val_rec: 0.7213883677298312, val_f1: 0.7209756097560976\n",
      "Epoch 6 test_acc: 0.6834208552138035, test_prec: 0.6847998183756466, test_rec: 0.6834208552138035, test_f1: 0.6828099783972681\n",
      "Saving as best model...\n",
      "Loss 5.750125177727995\n",
      "Epoch 7 train_acc: 0.9427633122214403, train_prec: 0.9434857883324046, train_rec: 0.9427633122214403, train_f1: 0.9427402801022363\n",
      "Epoch 7 val_acc: 0.701688555347092, val_prec: 0.703072446509518, val_rec: 0.701688555347092, val_f1: 0.7011794573438408\n",
      "Epoch 7 test_acc: 0.6946736684171043, test_prec: 0.6966380269652211, test_rec: 0.6946736684171043, test_f1: 0.6938887594070569\n",
      "Loss 7.325214062095225\n",
      "Epoch 8 train_acc: 0.952146375791696, train_prec: 0.9522038324271069, train_rec: 0.952146375791696, train_f1: 0.952144922221903\n",
      "Epoch 8 val_acc: 0.7063789868667918, val_prec: 0.7065425696812558, val_rec: 0.7063789868667918, val_f1: 0.7063208380019558\n",
      "Epoch 8 test_acc: 0.7014253563390848, test_prec: 0.7015663283217398, test_rec: 0.7014253563390848, test_f1: 0.7013678776344812\n",
      "Loss 6.467051180585392\n",
      "Epoch 9 train_acc: 0.9493314567206192, train_prec: 0.9504031547788634, train_rec: 0.9493314567206192, train_f1: 0.9493009922285081\n",
      "Epoch 9 val_acc: 0.7148217636022514, val_prec: 0.7192800911204291, val_rec: 0.7148217636022514, val_f1: 0.713364821595995\n",
      "Epoch 9 test_acc: 0.7014253563390848, test_prec: 0.7045485010358582, test_rec: 0.7014253563390848, test_f1: 0.700305149923553\n",
      "Loss 7.034155418077881\n",
      "Epoch 10 train_acc: 0.9619985925404645, train_prec: 0.9621615166795845, train_rec: 0.9619985925404645, train_f1: 0.9619953301948788\n",
      "Epoch 10 val_acc: 0.7120075046904315, val_prec: 0.7133964002267574, val_rec: 0.7120075046904315, val_f1: 0.7115381394737978\n",
      "Epoch 10 test_acc: 0.6864216054013503, test_prec: 0.6874590597874977, test_rec: 0.6864216054013503, test_f1: 0.6859709317191598\n",
      "Loss 6.800055425495216\n",
      "Epoch 11 train_acc: 0.9688013136288999, train_prec: 0.9690700247301364, train_rec: 0.9688013136288999, train_f1: 0.9687969353283405\n",
      "Epoch 11 val_acc: 0.7110694183864915, val_prec: 0.7135987176061982, val_rec: 0.7110694183864915, val_f1: 0.7102115460656385\n",
      "Epoch 11 test_acc: 0.6909227306826706, test_prec: 0.69528503769167, test_rec: 0.6909227306826706, test_f1: 0.6891557631025971\n",
      "Loss 7.950257185504883\n",
      "Epoch 12 train_acc: 0.9756040347173353, train_prec: 0.9757716919843984, train_rec: 0.9756040347173353, train_f1: 0.975601940372021\n",
      "Epoch 12 val_acc: 0.7176360225140713, val_prec: 0.718282212054454, val_rec: 0.7176360225140713, val_f1: 0.7174268944047415\n",
      "Epoch 12 test_acc: 0.68192048012003, test_prec: 0.6833853621181046, test_rec: 0.68192048012003, test_f1: 0.681263931477848\n",
      "Loss 7.210463355767096\n",
      "Epoch 13 train_acc: 0.970677926342951, train_prec: 0.9710149383607013, train_rec: 0.970677926342951, train_f1: 0.9706727752208197\n",
      "Epoch 13 val_acc: 0.7091932457786116, val_prec: 0.7112035112035112, val_rec: 0.7091932457786116, val_f1: 0.7084996101096281\n",
      "Epoch 13 test_acc: 0.6849212303075769, test_prec: 0.6865134272012665, test_rec: 0.6849212303075769, test_f1: 0.6842270660619152\n",
      "Loss 7.889256075016969\n",
      "Epoch 14 train_acc: 0.9798264133239503, train_prec: 0.9798606739006277, train_rec: 0.9798264133239503, train_f1: 0.9798260736354958\n",
      "Epoch 14 val_acc: 0.7120075046904315, val_prec: 0.7136689371363701, val_rec: 0.7120075046904315, val_f1: 0.7114465761492998\n",
      "Epoch 14 test_acc: 0.6946736684171043, test_prec: 0.6962972000379078, test_rec: 0.6946736684171043, test_f1: 0.6940223909898272\n",
      "Loss 7.085504414117025\n",
      "Epoch 15 train_acc: 0.9861599812338728, train_prec: 0.9861729125939012, train_rec: 0.9861599812338728, train_f1: 0.9861598807067192\n",
      "Epoch 15 val_acc: 0.724202626641651, val_prec: 0.724231041390163, val_rec: 0.724202626641651, val_f1: 0.7241938890453394\n",
      "Epoch 15 test_acc: 0.6811702925731433, test_prec: 0.681324458919542, test_rec: 0.6811702925731433, test_f1: 0.6811089158377355\n",
      "Saving as best model...\n",
      "Loss 9.079189332217465\n",
      "Epoch 16 train_acc: 0.990382359840488, train_prec: 0.9904394884111971, train_rec: 0.990382359840488, train_f1: 0.990382092047043\n",
      "Epoch 16 val_acc: 0.7223264540337712, val_prec: 0.7232348294472543, val_rec: 0.7223264540337712, val_f1: 0.7220436927413673\n",
      "Epoch 16 test_acc: 0.6946736684171043, test_prec: 0.6961909800227025, test_rec: 0.6946736684171042, test_f1: 0.6940641483592889\n",
      "Loss 10.263713924372464\n",
      "Epoch 17 train_acc: 0.9730236922355149, train_prec: 0.9736741426458515, train_rec: 0.9730236922355149, train_f1: 0.973014307503146\n",
      "Epoch 17 val_acc: 0.7082551594746717, val_prec: 0.7122363323288852, val_rec: 0.7082551594746717, val_f1: 0.7068805607646854\n",
      "Epoch 17 test_acc: 0.6796699174793699, test_prec: 0.6825299860638295, test_rec: 0.6796699174793699, test_f1: 0.6784382997817906\n",
      "Loss 8.910611385558976\n",
      "Epoch 18 train_acc: 0.9873328641801549, train_prec: 0.9875801907660552, train_rec: 0.9873328641801549, train_f1: 0.9873312915027393\n",
      "Epoch 18 val_acc: 0.698874296435272, val_prec: 0.7048453996983409, val_rec: 0.698874296435272, val_f1: 0.6966637856525496\n",
      "Epoch 18 test_acc: 0.6781695423855963, test_prec: 0.6835951274570004, test_rec: 0.6781695423855963, test_f1: 0.6757349626172829\n",
      "Loss 9.420164728724501\n",
      "Epoch 19 train_acc: 0.9404175463288764, train_prec: 0.9459765391945008, train_rec: 0.9404175463288764, train_f1: 0.940230465287265\n",
      "Epoch 19 val_acc: 0.6754221388367729, val_prec: 0.6958134417791398, val_rec: 0.6754221388367729, val_f1: 0.6667461771710885\n",
      "Epoch 19 test_acc: 0.6744186046511628, test_prec: 0.692914402554349, test_rec: 0.6744186046511628, test_f1: 0.6664933134896761\n",
      "Loss 8.000374265221073\n",
      "Epoch 20 train_acc: 0.9582453671123622, train_prec: 0.9613918065740535, train_rec: 0.9582453671123622, train_f1: 0.9581744816459723\n",
      "Epoch 20 val_acc: 0.702626641651032, val_prec: 0.7283850695105695, val_rec: 0.702626641651032, val_f1: 0.6939985420826486\n",
      "Epoch 20 test_acc: 0.6744186046511628, test_prec: 0.6935670033453871, test_rec: 0.6744186046511628, test_f1: 0.6660909793268049\n",
      "Loss 8.626825033506721\n",
      "Epoch 21 train_acc: 0.9960121979826413, train_prec: 0.9960123067184468, train_rec: 0.9960121979826413, train_f1: 0.9960121975437742\n",
      "Epoch 21 val_acc: 0.7101313320825516, val_prec: 0.710137989298789, val_rec: 0.7101313320825516, val_f1: 0.7101290362890913\n",
      "Epoch 21 test_acc: 0.6856714178544636, test_prec: 0.6856741182661776, test_rec: 0.6856714178544636, test_f1: 0.6856692950561624\n",
      "Loss 10.516867385508709\n",
      "Epoch 22 train_acc: 0.9962467745718977, train_prec: 0.9962485205474726, train_rec: 0.9962467745718977, train_f1: 0.996246770441379\n",
      "Epoch 22 val_acc: 0.7035647279549718, val_prec: 0.7040502729232604, val_rec: 0.7035647279549718, val_f1: 0.7033882783882784\n",
      "Epoch 22 test_acc: 0.6931732933233309, test_prec: 0.6934115474752992, test_rec: 0.6931732933233309, test_f1: 0.6930858952842697\n",
      "Loss 11.654722013381758\n",
      "Epoch 23 train_acc: 0.9983579638752053, train_prec: 0.998358951637087, train_rec: 0.9983579638752053, train_f1: 0.9983579633330751\n",
      "Epoch 23 val_acc: 0.7073170731707317, val_prec: 0.7078115682766847, val_rec: 0.7073170731707317, val_f1: 0.7071428571428571\n",
      "Epoch 23 test_acc: 0.6886721680420105, test_prec: 0.6890401966089953, test_rec: 0.6886721680420105, test_f1: 0.6885298351283877\n",
      "Loss 12.534750089284307\n",
      "Epoch 24 train_acc: 0.9981233872859488, train_prec: 0.998127335566233, train_rec: 0.9981233872859488, train_f1: 0.9981233841880599\n",
      "Epoch 24 val_acc: 0.7091932457786116, val_prec: 0.7092669082253187, val_rec: 0.7091932457786116, val_f1: 0.7091676523600315\n",
      "Epoch 24 test_acc: 0.6984246061515379, test_prec: 0.698484875775699, test_rec: 0.6984246061515379, test_f1: 0.6983981271557252\n",
      "Loss 13.365951919532211\n",
      "Epoch 25 train_acc: 0.9967159277504105, train_prec: 0.996716365791518, train_rec: 0.9967159277504105, train_f1: 0.9967159273889906\n",
      "Epoch 25 val_acc: 0.7073170731707317, val_prec: 0.7073170731707317, val_rec: 0.7073170731707317, val_f1: 0.7073170731707317\n",
      "Epoch 25 test_acc: 0.6946736684171043, test_prec: 0.6947122646923128, test_rec: 0.6946736684171043, test_f1: 0.6946612960698703\n",
      "Loss 12.964266790380009\n",
      "Epoch 26 train_acc: 0.9741965751817968, train_prec: 0.9752846974080669, train_rec: 0.9741965751817968, train_f1: 0.9741816493794243\n",
      "Epoch 26 val_acc: 0.7195121951219512, val_prec: 0.7365384615384616, val_rec: 0.7195121951219512, val_f1: 0.7143722550355899\n",
      "Epoch 26 test_acc: 0.6774193548387096, test_prec: 0.6901396752963463, test_rec: 0.6774193548387096, test_f1: 0.6719904916262757\n",
      "Loss 12.87091739082418\n",
      "Epoch 27 train_acc: 0.9960121979826413, train_prec: 0.9960149251719261, train_rec: 0.9960121979826413, train_f1: 0.9960121913996235\n",
      "Epoch 27 val_acc: 0.7213883677298312, val_prec: 0.7214827022763128, val_rec: 0.7213883677298312, val_f1: 0.7213586978045915\n",
      "Epoch 27 test_acc: 0.695423855963991, test_prec: 0.6956068384303324, test_rec: 0.695423855963991, test_f1: 0.6953587070134911\n",
      "Loss 11.796216099620457\n",
      "Epoch 28 train_acc: 0.9969505043396669, train_prec: 0.9969558664148217, train_rec: 0.9969505043396669, train_f1: 0.9969504972919654\n",
      "Epoch 28 val_acc: 0.7185741088180112, val_prec: 0.718586419687897, val_rec: 0.7185741088180112, val_f1: 0.7185701462590421\n",
      "Epoch 28 test_acc: 0.6961740435108777, test_prec: 0.6962785854026816, test_rec: 0.6961740435108777, test_f1: 0.6961381321571082\n",
      "Loss 11.788666007870962\n",
      "Epoch 29 train_acc: 0.9990616936429745, train_prec: 0.999062133233025, train_rec: 0.9990616936429745, train_f1: 0.9990616935397116\n",
      "Epoch 29 val_acc: 0.7157598499061913, val_prec: 0.7160343709468224, val_rec: 0.7157598499061913, val_f1: 0.715669523193324\n",
      "Epoch 29 test_acc: 0.6961740435108777, test_prec: 0.6961790440181227, test_rec: 0.6961740435108777, test_f1: 0.6961730175830665\n",
      "Loss 12.052400324232972\n",
      "Epoch 30 train_acc: 0.9990616936429745, train_prec: 0.999062133233025, train_rec: 0.9990616936429745, train_f1: 0.9990616935397116\n",
      "Epoch 30 val_acc: 0.7185741088180112, val_prec: 0.719018175533397, val_rec: 0.7185741088180112, val_f1: 0.7184313863600345\n",
      "Epoch 30 test_acc: 0.695423855963991, test_prec: 0.6955053200747824, test_rec: 0.695423855963991, test_f1: 0.6953878553525937\n",
      "Loss 12.3410755996605\n",
      "Epoch 31 train_acc: 0.9988271170537181, train_prec: 0.9988298625662327, train_rec: 0.9988271170537181, train_f1: 0.9988271157629315\n",
      "Epoch 31 val_acc: 0.724202626641651, val_prec: 0.7244586189276337, val_rec: 0.724202626641651, val_f1: 0.7241239683389199\n",
      "Epoch 31 test_acc: 0.6969242310577645, test_prec: 0.6969253220161165, test_rec: 0.6969242310577645, test_f1: 0.6969232076602285\n",
      "Loss 12.583930766099538\n",
      "Epoch 32 train_acc: 0.9992962702322308, train_prec: 0.9992972595430571, train_rec: 0.9992962702322308, train_f1: 0.9992962699998894\n",
      "Epoch 32 val_acc: 0.7195121951219512, val_prec: 0.7198534803700362, val_rec: 0.7195121951219512, val_f1: 0.719403300423007\n",
      "Epoch 32 test_acc: 0.7036759189797449, test_prec: 0.7037490996026189, test_rec: 0.7036759189797449, test_f1: 0.7036455642798921\n",
      "Loss 12.865042770122812\n",
      "Epoch 33 train_acc: 0.9990616936429745, train_prec: 0.9990634515939945, train_rec: 0.9990616936429745, train_f1: 0.9990616930233971\n",
      "Epoch 33 val_acc: 0.7157598499061913, val_prec: 0.7166942417557555, val_rec: 0.7157598499061913, val_f1: 0.7154531062934587\n",
      "Epoch 33 test_acc: 0.6961740435108777, test_prec: 0.6967045545909755, test_rec: 0.6961740435108777, test_f1: 0.6959584419033155\n",
      "Loss 12.913810327573826\n",
      "Epoch 34 train_acc: 0.9990616936429745, train_prec: 0.9990634515939945, train_rec: 0.9990616936429745, train_f1: 0.9990616930233971\n",
      "Epoch 34 val_acc: 0.7157598499061913, val_prec: 0.7166942417557555, val_rec: 0.7157598499061913, val_f1: 0.7154531062934587\n",
      "Epoch 34 test_acc: 0.6939234808702176, test_prec: 0.6944177391699808, test_rec: 0.6939234808702176, test_f1: 0.6937183570079527\n",
      "Loss 12.989830848986204\n",
      "Epoch 35 train_acc: 0.9990616936429745, train_prec: 0.9990634515939945, train_rec: 0.9990616936429745, train_f1: 0.9990616930233971\n",
      "Epoch 35 val_acc: 0.7185741088180112, val_prec: 0.7198120988781863, val_rec: 0.7185741088180112, val_f1: 0.7181772997895582\n",
      "Epoch 35 test_acc: 0.6976744186046512, test_prec: 0.69841135199054, test_rec: 0.6976744186046512, test_f1: 0.697381140862992\n",
      "Loss 13.000796133377687\n",
      "Epoch 36 train_acc: 0.9990616936429745, train_prec: 0.9990634515939945, train_rec: 0.9990616936429745, train_f1: 0.9990616930233971\n",
      "Epoch 36 val_acc: 0.7157598499061913, val_prec: 0.7173088377105904, val_rec: 0.7157598499061913, val_f1: 0.715252426320007\n",
      "Epoch 36 test_acc: 0.6999249812453113, test_prec: 0.7007078780924482, test_rec: 0.6999249812453113, test_f1: 0.6996196723599505\n",
      "Loss 13.098068247518652\n",
      "Epoch 37 train_acc: 0.9990616936429745, train_prec: 0.9990634515939945, train_rec: 0.9990616936429745, train_f1: 0.9990616930233971\n",
      "Epoch 37 val_acc: 0.7148217636022514, val_prec: 0.7160385006141833, val_rec: 0.7148217636022514, val_f1: 0.7144196637867525\n",
      "Epoch 37 test_acc: 0.6999249812453113, test_prec: 0.7004978129311844, test_rec: 0.6999249812453113, test_f1: 0.6996998641387842\n",
      "Loss 13.180322756158802\n",
      "Epoch 38 train_acc: 0.9992962702322308, train_prec: 0.9992972595430571, train_rec: 0.9992962702322308, train_f1: 0.9992962699998894\n",
      "Epoch 38 val_acc: 0.7110694183864915, val_prec: 0.7119318001081528, val_rec: 0.7110694183864915, val_f1: 0.7107751937984496\n",
      "Epoch 38 test_acc: 0.6999249812453113, test_prec: 0.7004351295032266, test_rec: 0.6999249812453113, test_f1: 0.6997238794195617\n",
      "Loss 13.25190505364464\n",
      "Epoch 39 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 39 val_acc: 0.7120075046904315, val_prec: 0.7127271057613519, val_rec: 0.7120075046904315, val_f1: 0.7117637474182994\n",
      "Epoch 39 test_acc: 0.6991747936984246, test_prec: 0.6995427722707757, test_rec: 0.6991747936984246, test_f1: 0.6990274290866605\n",
      "Loss 13.312518938863064\n",
      "Epoch 40 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 40 val_acc: 0.7101313320825516, val_prec: 0.7106719367588932, val_rec: 0.7101313320825516, val_f1: 0.7099452549120442\n",
      "Epoch 40 test_acc: 0.6999249812453113, test_prec: 0.7002689972463798, test_rec: 0.6999249812453113, test_f1: 0.6997877884919461\n",
      "Loss 13.36870340311914\n",
      "Epoch 41 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 41 val_acc: 0.7091932457786116, val_prec: 0.7096922159533967, val_rec: 0.7091932457786116, val_f1: 0.7090201465201466\n",
      "Epoch 41 test_acc: 0.7014253563390848, test_prec: 0.7017236113682099, test_rec: 0.7014253563390848, test_f1: 0.7013073492902548\n",
      "Loss 13.37356436601702\n",
      "Epoch 42 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 42 val_acc: 0.7082551594746717, val_prec: 0.7087143340953348, val_rec: 0.7082551594746717, val_f1: 0.7080946104315194\n",
      "Epoch 42 test_acc: 0.700675168792198, test_prec: 0.700949488226484, test_rec: 0.700675168792198, test_f1: 0.7005656317376415\n",
      "Loss 13.39106738230861\n",
      "Epoch 43 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 43 val_acc: 0.7073170731707317, val_prec: 0.7077382694973422, val_rec: 0.7073170731707317, val_f1: 0.707168641814436\n",
      "Epoch 43 test_acc: 0.6999249812453113, test_prec: 0.7001356179266941, test_rec: 0.6999249812453113, test_f1: 0.6998395042251747\n",
      "Loss 13.403553942816886\n",
      "Epoch 44 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 44 val_acc: 0.7073170731707317, val_prec: 0.7077382694973422, val_rec: 0.7073170731707317, val_f1: 0.707168641814436\n",
      "Epoch 44 test_acc: 0.6999249812453113, test_prec: 0.7000984127357295, test_rec: 0.6999249812453113, test_f1: 0.6998540354546791\n",
      "Loss 13.408118174263555\n",
      "Epoch 45 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 45 val_acc: 0.7082551594746717, val_prec: 0.7086436732966568, val_rec: 0.7082551594746717, val_f1: 0.7081192822498498\n",
      "Epoch 45 test_acc: 0.7014253563390848, test_prec: 0.701600173929134, test_rec: 0.7014253563390848, test_f1: 0.7013547652774057\n",
      "Loss 13.406965815500655\n",
      "Epoch 46 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 46 val_acc: 0.7082551594746717, val_prec: 0.7086436732966568, val_rec: 0.7082551594746717, val_f1: 0.7081192822498498\n",
      "Epoch 46 test_acc: 0.7021755438859715, test_prec: 0.702333609231319, test_rec: 0.7021755438859715, test_f1: 0.7021118377254923\n",
      "Loss 13.401123378035551\n",
      "Epoch 47 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 47 val_acc: 0.7073170731707317, val_prec: 0.70767088027362, val_rec: 0.7073170731707317, val_f1: 0.7071923597025017\n",
      "Epoch 47 test_acc: 0.7021755438859715, test_prec: 0.702333609231319, test_rec: 0.7021755438859715, test_f1: 0.7021118377254923\n",
      "Loss 13.391249193926699\n",
      "Epoch 48 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 48 val_acc: 0.7073170731707317, val_prec: 0.70767088027362, val_rec: 0.7073170731707317, val_f1: 0.7071923597025017\n",
      "Epoch 48 test_acc: 0.7029257314328582, test_prec: 0.7031019351225385, test_rec: 0.7029257314328582, test_f1: 0.7028554951001323\n",
      "Loss 13.37794456650865\n",
      "Epoch 49 train_acc: 0.9995308468214872, train_prec: 0.9995312867213264, train_rec: 0.9995308468214872, train_f1: 0.9995308467698557\n",
      "Epoch 49 val_acc: 0.7073170731707317, val_prec: 0.7077382694973422, val_rec: 0.7073170731707317, val_f1: 0.707168641814436\n",
      "Epoch 49 test_acc: 0.7044261065266316, test_prec: 0.7046036963159429, test_rec: 0.7044261065266316, test_f1: 0.7043562249228589\n",
      "Loss 13.361819973724552\n",
      "Epoch 0 train_acc: 0.4813511611541168, train_prec: 0.48013004529258924, train_rec: 0.4813511611541168, train_f1: 0.4730678047651271\n",
      "Epoch 0 val_acc: 0.46716697936210133, val_prec: 0.4643429196181797, val_rec: 0.46716697936210133, val_f1: 0.45640370029878197\n",
      "Epoch 0 test_acc: 0.47186796699174793, test_prec: 0.4691365992445965, test_rec: 0.47186796699174793, test_f1: 0.4603781467362811\n",
      "Saving as best model...\n",
      "Loss 1.6033818587454676\n",
      "Epoch 1 train_acc: 0.49941355852685904, train_prec: 0.47961614171494826, train_rec: 0.49941355852685904, train_f1: 0.34009734145224296\n",
      "Epoch 1 val_acc: 0.49906191369606, val_prec: 0.41643148322358103, val_rec: 0.49906191369606, val_f1: 0.3345724385590843\n",
      "Epoch 1 test_acc: 0.49962490622655664, test_prec: 0.49981174645583026, test_rec: 0.49962490622655664, test_f1: 0.33687576088233745\n",
      "Loss 2.8945309090913582\n",
      "Epoch 2 train_acc: 0.5547736335913676, train_prec: 0.5607038660562864, train_rec: 0.5547736335913676, train_f1: 0.5435487378734274\n",
      "Epoch 2 val_acc: 0.5318949343339587, val_prec: 0.5360524891774893, val_rec: 0.5318949343339587, val_f1: 0.5179989035832891\n",
      "Epoch 2 test_acc: 0.5453863465866466, test_prec: 0.5521549256055891, test_rec: 0.5453863465866466, test_f1: 0.5304839000907418\n",
      "Saving as best model...\n",
      "Loss 4.159197096142644\n",
      "Epoch 3 train_acc: 0.6199859254046446, train_prec: 0.6517738170095665, train_rec: 0.6199859254046446, train_f1: 0.5989411053225387\n",
      "Epoch 3 val_acc: 0.5844277673545967, val_prec: 0.6070453084832904, val_rec: 0.5844277673545967, val_f1: 0.5612520683693027\n",
      "Epoch 3 test_acc: 0.582145536384096, test_prec: 0.6044313515000881, test_rec: 0.582145536384096, test_f1: 0.5588251656370928\n",
      "Saving as best model...\n",
      "Loss 2.0260077226477278\n",
      "Epoch 4 train_acc: 0.622566267886465, train_prec: 0.6888319071012069, train_rec: 0.622566267886465, train_f1: 0.586214527708343\n",
      "Epoch 4 val_acc: 0.5806754221388368, val_prec: 0.6437108101329321, val_rec: 0.5806754221388368, val_f1: 0.5290304789636143\n",
      "Epoch 4 test_acc: 0.5596399099774944, test_prec: 0.6056449990087424, test_rec: 0.5596399099774944, test_f1: 0.5062197743790051\n",
      "Loss 1.8614926294727414\n",
      "Epoch 5 train_acc: 0.5819845179451091, train_prec: 0.6299975819200252, train_rec: 0.5819845179451091, train_f1: 0.5393764927014695\n",
      "Epoch 5 val_acc: 0.5778611632270169, val_prec: 0.6405724708140296, val_rec: 0.5778611632270169, val_f1: 0.52487074344803\n",
      "Epoch 5 test_acc: 0.5573893473368342, test_prec: 0.6082220114042121, test_rec: 0.5573893473368342, test_f1: 0.4988920136731833\n",
      "Loss 1.9145610895374783\n",
      "Epoch 6 train_acc: 0.7077175697865353, train_prec: 0.7216112761313704, train_rec: 0.7077175697865353, train_f1: 0.703049239216054\n",
      "Epoch 6 val_acc: 0.6200750469043153, val_prec: 0.6280480480480481, val_rec: 0.6200750469043153, val_f1: 0.6140674570716756\n",
      "Epoch 6 test_acc: 0.5986496624156039, test_prec: 0.6090544080993187, test_rec: 0.5986496624156039, test_f1: 0.5889771376210733\n",
      "Saving as best model...\n",
      "Loss 1.7939310185964672\n",
      "Epoch 7 train_acc: 0.6952850105559465, train_prec: 0.7055648225178424, train_rec: 0.6952850105559465, train_f1: 0.6914132877654278\n",
      "Epoch 7 val_acc: 0.6050656660412758, val_prec: 0.6150477952513105, val_rec: 0.6050656660412758, val_f1: 0.5963090994299758\n",
      "Epoch 7 test_acc: 0.5986496624156039, test_prec: 0.6090544080993187, test_rec: 0.5986496624156039, test_f1: 0.5889771376210733\n",
      "Loss 2.4455923059349436\n",
      "Epoch 8 train_acc: 0.5148956134177809, train_prec: 0.5508012926913939, train_rec: 0.5148956134177809, train_f1: 0.4104121238063064\n",
      "Epoch 8 val_acc: 0.5046904315196998, val_prec: 0.5188619152098521, val_rec: 0.5046904315196998, val_f1: 0.39013903817482054\n",
      "Epoch 8 test_acc: 0.5063765941485371, test_prec: 0.5274854026508232, test_rec: 0.5063765941485371, test_f1: 0.3913910968891241\n",
      "Loss 2.0147852658920375\n",
      "Epoch 9 train_acc: 0.7363359136758152, train_prec: 0.7391759985927809, train_rec: 0.7363359136758152, train_f1: 0.7355560883203652\n",
      "Epoch 9 val_acc: 0.626641651031895, val_prec: 0.6322755699679031, val_rec: 0.626641651031895, val_f1: 0.6226233171092705\n",
      "Epoch 9 test_acc: 0.6054013503375844, test_prec: 0.6095110572339616, test_rec: 0.6054013503375844, test_f1: 0.6015825698193731\n",
      "Saving as best model...\n",
      "Loss 2.6959451390403055\n",
      "Epoch 10 train_acc: 0.775744780670889, train_prec: 0.7773034029380094, train_rec: 0.775744780670889, train_f1: 0.7754264506161107\n",
      "Epoch 10 val_acc: 0.648217636022514, val_prec: 0.6491886337868481, val_rec: 0.648217636022514, val_f1: 0.6476443071748346\n",
      "Epoch 10 test_acc: 0.6421605401350338, test_prec: 0.643467572191061, test_rec: 0.6421605401350338, test_f1: 0.641372223464993\n",
      "Saving as best model...\n",
      "Loss 5.036920430279413\n",
      "Epoch 11 train_acc: 0.675815153647666, train_prec: 0.7561802837318357, train_rec: 0.675815153647666, train_f1: 0.648191822112213\n",
      "Epoch 11 val_acc: 0.6181988742964353, val_prec: 0.7069099378881988, val_rec: 0.6181988742964353, val_f1: 0.572362267200891\n",
      "Epoch 11 test_acc: 0.5843960990247562, test_prec: 0.656519854471668, test_rec: 0.5843960990247562, test_f1: 0.5305528625834198\n",
      "Loss 2.607150357205513\n",
      "Epoch 12 train_acc: 0.8045977011494253, train_prec: 0.8098245078951428, train_rec: 0.8045977011494253, train_f1: 0.8037661778818267\n",
      "Epoch 12 val_acc: 0.6547842401500938, val_prec: 0.6624801853432508, val_rec: 0.6547842401500938, val_f1: 0.6506474307596402\n",
      "Epoch 12 test_acc: 0.6504126031507877, test_prec: 0.6587248528371084, test_rec: 0.6504126031507877, test_f1: 0.6458383001896574\n",
      "Saving as best model...\n",
      "Loss 3.125085343838379\n",
      "Epoch 13 train_acc: 0.8001407459535539, train_prec: 0.8127421782895028, train_rec: 0.8001407459535539, train_f1: 0.7981008162052093\n",
      "Epoch 13 val_acc: 0.6332082551594747, val_prec: 0.6465828452790432, val_rec: 0.6332082551594747, val_f1: 0.6246461972110029\n",
      "Epoch 13 test_acc: 0.6564141035258815, test_prec: 0.6726609074626593, test_rec: 0.6564141035258815, test_f1: 0.6482164062863061\n",
      "Loss 2.7232465185469388\n",
      "Epoch 14 train_acc: 0.8362655406990382, train_prec: 0.836976119129017, train_rec: 0.8362655406990382, train_f1: 0.8361780816042196\n",
      "Epoch 14 val_acc: 0.6294559099437148, val_prec: 0.6297413428160189, val_rec: 0.6294559099437148, val_f1: 0.6292519971718653\n",
      "Epoch 14 test_acc: 0.654913728432108, test_prec: 0.655958330936731, test_rec: 0.654913728432108, test_f1: 0.6543570247269611\n",
      "Loss 3.564763051636668\n",
      "Epoch 15 train_acc: 0.7705840957072484, train_prec: 0.8014307806764609, train_rec: 0.7705840957072484, train_f1: 0.7645488985548184\n",
      "Epoch 15 val_acc: 0.6425891181988743, val_prec: 0.6832211608047474, val_rec: 0.6425891181988743, val_f1: 0.6216107438824993\n",
      "Epoch 15 test_acc: 0.6159039759939985, test_prec: 0.65950444039197, test_rec: 0.6159039759939985, test_f1: 0.587903698613729\n",
      "Loss 3.534584863989988\n",
      "Epoch 16 train_acc: 0.8374384236453202, train_prec: 0.8439324693832, train_rec: 0.8374384236453202, train_f1: 0.8366641887787831\n",
      "Epoch 16 val_acc: 0.6819887429643527, val_prec: 0.689931963792394, val_rec: 0.6819887429643527, val_f1: 0.6786286933902488\n",
      "Epoch 16 test_acc: 0.6646661665416355, test_prec: 0.672206211490761, test_rec: 0.6646661665416355, test_f1: 0.6610069283304174\n",
      "Saving as best model...\n",
      "Loss 4.521121800062303\n",
      "Epoch 17 train_acc: 0.8461177574478067, train_prec: 0.8498482599374342, train_rec: 0.8461177574478067, train_f1: 0.8457087113035547\n",
      "Epoch 17 val_acc: 0.6744840525328331, val_prec: 0.6820997178628108, val_rec: 0.6744840525328331, val_f1: 0.6710447097534405\n",
      "Epoch 17 test_acc: 0.6676669167291823, test_prec: 0.6718948412118962, test_rec: 0.6676669167291823, test_f1: 0.6655720163060503\n",
      "Loss 4.496759776696981\n",
      "Epoch 18 train_acc: 0.847525216983345, train_prec: 0.8598253370977091, train_rec: 0.847525216983345, train_f1: 0.8462149407621781\n",
      "Epoch 18 val_acc: 0.6660412757973734, val_prec: 0.6848453499171203, val_rec: 0.6660412757973734, val_f1: 0.6573263407304669\n",
      "Epoch 18 test_acc: 0.6571642910727682, test_prec: 0.6705211160245828, test_rec: 0.6571642910727682, test_f1: 0.6502435786116801\n",
      "Loss 4.3356804488031155\n",
      "Epoch 19 train_acc: 0.88036593947924, train_prec: 0.8843152074895902, train_rec: 0.88036593947924, train_f1: 0.8800594451374146\n",
      "Epoch 19 val_acc: 0.6866791744840526, val_prec: 0.6956284374734869, val_rec: 0.6866791744840526, val_f1: 0.6830544082039595\n",
      "Epoch 19 test_acc: 0.6841710427606902, test_prec: 0.6932699208528482, test_rec: 0.6841710427606902, test_f1: 0.6803625431854767\n",
      "Saving as best model...\n",
      "Loss 4.717665139826644\n",
      "Epoch 20 train_acc: 0.900070372976777, train_prec: 0.9007858889809257, train_rec: 0.900070372976777, train_f1: 0.900026308821283\n",
      "Epoch 20 val_acc: 0.6801125703564728, val_prec: 0.6819112627986348, val_rec: 0.6801125703564728, val_f1: 0.6793198694010923\n",
      "Epoch 20 test_acc: 0.6796699174793699, test_prec: 0.6799555197887429, test_rec: 0.6796699174793699, test_f1: 0.6795335688162077\n",
      "Loss 4.385275008904771\n",
      "Epoch 21 train_acc: 0.9143795449214168, train_prec: 0.9148654763671495, train_rec: 0.9143795449214168, train_f1: 0.9143540864497036\n",
      "Epoch 21 val_acc: 0.700750469043152, val_prec: 0.7020656179138323, val_rec: 0.700750469043152, val_f1: 0.7002627573033925\n",
      "Epoch 21 test_acc: 0.6864216054013503, test_prec: 0.6893156032735045, test_rec: 0.6864216054013503, test_f1: 0.6852451072061434\n",
      "Saving as best model...\n",
      "Loss 5.09984961488513\n",
      "Epoch 22 train_acc: 0.9171944639924936, train_prec: 0.9174729223057624, train_rec: 0.9171944639924936, train_f1: 0.9171809290988439\n",
      "Epoch 22 val_acc: 0.699812382739212, val_prec: 0.6998236368820691, val_rec: 0.699812382739212, val_f1: 0.699808156009645\n",
      "Epoch 22 test_acc: 0.6864216054013503, test_prec: 0.6868636174001873, test_rec: 0.6864216054013503, test_f1: 0.6862464464042277\n",
      "Loss 4.921941699732765\n",
      "Epoch 23 train_acc: 0.915083274689186, train_prec: 0.9167812349206265, train_rec: 0.915083274689186, train_f1: 0.9149973988486048\n",
      "Epoch 23 val_acc: 0.6923076923076923, val_prec: 0.6933969110301641, val_rec: 0.6923076923076923, val_f1: 0.6918738477699171\n",
      "Epoch 23 test_acc: 0.6789197299324832, test_prec: 0.6793742633368666, test_rec: 0.6789197299324832, test_f1: 0.6787045509789308\n",
      "Loss 6.237866825329917\n",
      "Epoch 24 train_acc: 0.9376026272577996, train_prec: 0.9379272768848922, train_rec: 0.9376026272577996, train_f1: 0.9375912741140167\n",
      "Epoch 24 val_acc: 0.6904315196998124, val_prec: 0.6913042402039654, val_rec: 0.6904315196998124, val_f1: 0.6900780575476188\n",
      "Epoch 24 test_acc: 0.6901725431357839, test_prec: 0.6905231508033994, test_rec: 0.6901725431357839, test_f1: 0.6900207686104509\n",
      "Loss 6.288390492709654\n",
      "Epoch 25 train_acc: 0.953319258737978, train_prec: 0.953341786823268, train_rec: 0.953319258737978, train_f1: 0.9533187193129942\n",
      "Epoch 25 val_acc: 0.6932457786116323, val_prec: 0.6934916539784582, val_rec: 0.6932457786116323, val_f1: 0.6931482973076466\n",
      "Epoch 25 test_acc: 0.7021755438859715, test_prec: 0.7027463310283051, test_rec: 0.7021755438859715, test_f1: 0.7019759580795605\n",
      "Loss 6.660611083790097\n",
      "Epoch 26 train_acc: 0.9516772226131832, train_prec: 0.9522525770970933, train_rec: 0.9516772226131832, train_f1: 0.9516620615198013\n",
      "Epoch 26 val_acc: 0.6969981238273921, val_prec: 0.6994419261033186, val_rec: 0.6969981238273921, val_f1: 0.6960670859538783\n",
      "Epoch 26 test_acc: 0.687171792948237, test_prec: 0.6883442499423116, test_rec: 0.687171792948237, test_f1: 0.6866670984976552\n",
      "Loss 5.550299339025119\n",
      "Epoch 27 train_acc: 0.9638752052545156, train_prec: 0.963979906882963, train_rec: 0.9638752052545156, train_f1: 0.963873233243388\n",
      "Epoch 27 val_acc: 0.6951219512195121, val_prec: 0.6957012935660623, val_rec: 0.6951219512195121, val_f1: 0.6948961484436577\n",
      "Epoch 27 test_acc: 0.6999249812453113, test_prec: 0.6999391580976427, test_rec: 0.6999249812453113, test_f1: 0.6999178882314284\n",
      "Loss 6.087057616769793\n",
      "Epoch 28 train_acc: 0.9683321604503871, train_prec: 0.9683694421185539, train_rec: 0.9683321604503871, train_f1: 0.968331564484201\n",
      "Epoch 28 val_acc: 0.7073170731707317, val_prec: 0.7073900749676924, val_rec: 0.7073170731707317, val_f1: 0.7072913146333222\n",
      "Epoch 28 test_acc: 0.7021755438859715, test_prec: 0.7022481315086001, test_rec: 0.7021755438859715, test_f1: 0.7021450354914358\n",
      "Saving as best model...\n",
      "Loss 5.965815478277871\n",
      "Epoch 29 train_acc: 0.9627023223082336, train_prec: 0.9627024282558948, train_rec: 0.9627023223082336, train_f1: 0.9627023223082336\n",
      "Epoch 29 val_acc: 0.6951219512195121, val_prec: 0.6951391235777853, val_rec: 0.6951219512195121, val_f1: 0.6951152437097993\n",
      "Epoch 29 test_acc: 0.6909227306826706, test_prec: 0.691038385905651, test_rec: 0.6909227306826706, test_f1: 0.6908809790939424\n",
      "Loss 6.1308682014164315\n",
      "Epoch 30 train_acc: 0.9739619985925405, train_prec: 0.9739621057793555, train_rec: 0.9739619985925405, train_f1: 0.9739619985925405\n",
      "Epoch 30 val_acc: 0.7082551594746717, val_prec: 0.7089620242434518, val_rec: 0.7082551594746717, val_f1: 0.7080082262120232\n",
      "Epoch 30 test_acc: 0.6939234808702176, test_prec: 0.6959866578663789, test_rec: 0.6939234808702176, test_f1: 0.6931367032685728\n",
      "Saving as best model...\n",
      "Loss 6.349587366245591\n",
      "Epoch 31 train_acc: 0.9779498006098991, train_prec: 0.97796497882781, train_rec: 0.9779498006098991, train_f1: 0.977949640448196\n",
      "Epoch 31 val_acc: 0.7129455909943715, val_prec: 0.7130926088350189, val_rec: 0.7129455909943715, val_f1: 0.7128960709759189\n",
      "Epoch 31 test_acc: 0.6939234808702176, test_prec: 0.6943825537414519, test_rec: 0.6939234808702176, test_f1: 0.6937525122797247\n",
      "Saving as best model...\n",
      "Loss 6.066445144855318\n",
      "Epoch 32 train_acc: 0.9793572601454374, train_prec: 0.9793678339521301, train_rec: 0.9793572601454374, train_f1: 0.9793571579147881\n",
      "Epoch 32 val_acc: 0.7157598499061913, val_prec: 0.7158517861167456, val_rec: 0.7157598499061913, val_f1: 0.7157295805885227\n",
      "Epoch 32 test_acc: 0.6916729182295573, test_prec: 0.6926471098840629, test_rec: 0.6916729182295573, test_f1: 0.6912973191243267\n",
      "Saving as best model...\n",
      "Loss 6.2523800335376665\n",
      "Epoch 33 train_acc: 0.9741965751817968, train_prec: 0.9745012718527509, train_rec: 0.9741965751817968, train_f1: 0.9741925109033881\n",
      "Epoch 33 val_acc: 0.702626641651032, val_prec: 0.708173007377405, val_rec: 0.702626641651032, val_f1: 0.7006326259958698\n",
      "Epoch 33 test_acc: 0.6931732933233309, test_prec: 0.6971466483152085, test_rec: 0.6931732933233309, test_f1: 0.6915902745382437\n",
      "Loss 5.916620495563149\n",
      "Epoch 34 train_acc: 0.9817030260380014, train_prec: 0.9817374167346055, train_rec: 0.9817030260380014, train_f1: 0.9817027179484726\n",
      "Epoch 34 val_acc: 0.7157598499061913, val_prec: 0.7161623642262661, val_rec: 0.7157598499061913, val_f1: 0.7156274679154485\n",
      "Epoch 34 test_acc: 0.700675168792198, test_prec: 0.7006881475525318, test_rec: 0.700675168792198, test_f1: 0.7006717996731758\n",
      "Loss 6.368971751353727\n",
      "Epoch 35 train_acc: 0.9826413323950269, train_prec: 0.9826685596973853, train_rec: 0.9826413323950269, train_f1: 0.9826411031487241\n",
      "Epoch 35 val_acc: 0.7120075046904315, val_prec: 0.7120082509644899, val_rec: 0.7120075046904315, val_f1: 0.7120072512551094\n",
      "Epoch 35 test_acc: 0.7021755438859715, test_prec: 0.7022830256224903, test_rec: 0.7021755438859715, test_f1: 0.7021403418922765\n",
      "Loss 6.142855268812741\n",
      "Epoch 36 train_acc: 0.9833450621627962, train_prec: 0.9833690277330898, train_rec: 0.9833450621627962, train_f1: 0.983344869704636\n",
      "Epoch 36 val_acc: 0.7110694183864915, val_prec: 0.7110813065655658, val_rec: 0.7110694183864915, val_f1: 0.7110653501592832\n",
      "Epoch 36 test_acc: 0.700675168792198, test_prec: 0.7009221722805778, test_rec: 0.700675168792198, test_f1: 0.7005899076245078\n",
      "Loss 6.120523547703843\n",
      "Epoch 37 train_acc: 0.983814215341309, train_prec: 0.9838450230171287, train_rec: 0.983814215341309, train_f1: 0.9838139730833162\n",
      "Epoch 37 val_acc: 0.7129455909943715, val_prec: 0.7130926088350189, val_rec: 0.7129455909943715, val_f1: 0.7128960709759189\n",
      "Epoch 37 test_acc: 0.7036759189797449, test_prec: 0.7038853197555881, test_rec: 0.7036759189797449, test_f1: 0.7036058616284028\n",
      "Loss 6.078026645730405\n",
      "Epoch 38 train_acc: 0.9842833685198217, train_prec: 0.9843073790115215, train_rec: 0.9842833685198217, train_f1: 0.9842831869043749\n",
      "Epoch 38 val_acc: 0.7148217636022514, val_prec: 0.715598890714147, val_rec: 0.7148217636022514, val_f1: 0.7145645502813275\n",
      "Epoch 38 test_acc: 0.6991747936984246, test_prec: 0.6991876964608125, test_rec: 0.6991747936984246, test_f1: 0.6991714076915879\n",
      "Loss 6.055144238714692\n",
      "Epoch 39 train_acc: 0.9847525216983345, train_prec: 0.9847705758100265, train_rec: 0.9847525216983345, train_f1: 0.9847523908116707\n",
      "Epoch 39 val_acc: 0.7063789868667918, val_prec: 0.7063971499380421, val_rec: 0.7063789868667918, val_f1: 0.706372527018976\n",
      "Epoch 39 test_acc: 0.6976744186046512, test_prec: 0.6983668298155986, test_rec: 0.6976744186046513, test_f1: 0.6974220613017621\n",
      "Loss 6.031756937225377\n",
      "Epoch 40 train_acc: 0.9842833685198217, train_prec: 0.9843142051625989, train_rec: 0.9842833685198217, train_f1: 0.9842831332837999\n",
      "Epoch 40 val_acc: 0.7110694183864915, val_prec: 0.7123882050827948, val_rec: 0.7110694183864915, val_f1: 0.7106202053249097\n",
      "Epoch 40 test_acc: 0.6976744186046512, test_prec: 0.6977074069117101, test_rec: 0.6976744186046512, test_f1: 0.6976591048547768\n",
      "Loss 5.905492388255669\n",
      "Epoch 41 train_acc: 0.9847525216983345, train_prec: 0.9847765546507375, train_rec: 0.9847525216983345, train_f1: 0.9847523455042442\n",
      "Epoch 41 val_acc: 0.7091932457786116, val_prec: 0.710261989421359, val_rec: 0.7091932457786116, val_f1: 0.7088232391586309\n",
      "Epoch 41 test_acc: 0.6976744186046512, test_prec: 0.6977973205490166, test_rec: 0.6976744186046512, test_f1: 0.6976223452901893\n",
      "Loss 5.903671205027124\n",
      "Epoch 42 train_acc: 0.9849870982875909, train_prec: 0.98501445364262, train_rec: 0.9849870982875909, train_f1: 0.9849869000205181\n",
      "Epoch 42 val_acc: 0.7063789868667918, val_prec: 0.7082859660092081, val_rec: 0.7063789868667918, val_f1: 0.7057053772640474\n",
      "Epoch 42 test_acc: 0.6946736684171043, test_prec: 0.6949843916716485, test_rec: 0.6946736684171043, test_f1: 0.6945437061081886\n",
      "Loss 5.87340221260816\n",
      "Epoch 43 train_acc: 0.9852216748768473, train_prec: 0.9852346158295647, train_rec: 0.9852216748768473, train_f1: 0.9852215854249355\n",
      "Epoch 43 val_acc: 0.7157598499061913, val_prec: 0.7158517861167456, val_rec: 0.7157598499061913, val_f1: 0.7157295805885227\n",
      "Epoch 43 test_acc: 0.6931732933233309, test_prec: 0.6932297239430367, test_rec: 0.6931732933233309, test_f1: 0.6931542978558136\n",
      "Loss 5.998846736685999\n",
      "Epoch 44 train_acc: 0.9856908280553601, train_prec: 0.9857037809352228, train_rec: 0.9856908280553601, train_f1: 0.9856907414431914\n",
      "Epoch 44 val_acc: 0.7110694183864915, val_prec: 0.7114296292378485, val_rec: 0.7110694183864915, val_f1: 0.7109463038088799\n",
      "Epoch 44 test_acc: 0.6991747936984246, test_prec: 0.6991798448693702, test_rec: 0.6991747936984246, test_f1: 0.6991737779032337\n",
      "Loss 5.930394252516886\n",
      "Epoch 45 train_acc: 0.9856908280553601, train_prec: 0.9857037809352228, train_rec: 0.9856908280553601, train_f1: 0.9856907414431914\n",
      "Epoch 45 val_acc: 0.7129455909943715, val_prec: 0.7130926088350189, val_rec: 0.7129455909943715, val_f1: 0.7128960709759189\n",
      "Epoch 45 test_acc: 0.6939234808702176, test_prec: 0.6939904975568354, test_rec: 0.6939234808702176, test_f1: 0.6939007417728648\n",
      "Loss 6.064520166393639\n",
      "Epoch 46 train_acc: 0.9856908280553601, train_prec: 0.9857037809352228, train_rec: 0.9856908280553601, train_f1: 0.9856907414431914\n",
      "Epoch 46 val_acc: 0.7110694183864915, val_prec: 0.7114296292378485, val_rec: 0.7110694183864915, val_f1: 0.7109463038088799\n",
      "Epoch 46 test_acc: 0.6991747936984246, test_prec: 0.6991755806639437, test_rec: 0.6991747936984246, test_f1: 0.6991747936984244\n",
      "Loss 6.024181918492564\n",
      "Epoch 47 train_acc: 0.9861599812338728, train_prec: 0.986168662574104, train_rec: 0.9861599812338728, train_f1: 0.9861599264011213\n",
      "Epoch 47 val_acc: 0.7138836772983115, val_prec: 0.7139205745669623, val_rec: 0.7138836772983115, val_f1: 0.7138713393475531\n",
      "Epoch 47 test_acc: 0.695423855963991, test_prec: 0.6957306066527273, test_rec: 0.695423855963991, test_f1: 0.6953124004069211\n",
      "Loss 6.097417735395259\n",
      "Epoch 48 train_acc: 0.9856908280553601, train_prec: 0.9857294528093331, train_rec: 0.9856908280553601, train_f1: 0.9856905587669353\n",
      "Epoch 48 val_acc: 0.7110694183864915, val_prec: 0.7114982381760273, val_rec: 0.7110694183864915, val_f1: 0.710922889996302\n",
      "Epoch 48 test_acc: 0.6984246061515379, test_prec: 0.6984559130871635, test_rec: 0.6984246061515379, test_f1: 0.6984151015129347\n",
      "Loss 6.020229901646897\n",
      "Epoch 49 train_acc: 0.9861599812338728, train_prec: 0.9861909337444793, train_rec: 0.9861599812338728, train_f1: 0.9861597740857341\n",
      "Epoch 49 val_acc: 0.7063789868667918, val_prec: 0.7069099378881988, val_rec: 0.7063789868667918, val_f1: 0.706190500930323\n",
      "Epoch 49 test_acc: 0.6946736684171043, test_prec: 0.694678643592499, test_rec: 0.6946736684171043, test_f1: 0.6946726374229828\n",
      "Loss 6.061682444569997\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.5, 1, 3]\n",
    "lr_losses = []\n",
    "for lr in lrs:\n",
    "    layers = [\n",
    "    FullyConnected(48, 7400),\n",
    "    Tanh(),\n",
    "    FullyConnected(8, 48),\n",
    "    Tanh(),\n",
    "    FullyConnected(1, 8),\n",
    "    ]\n",
    "    mymlp = MultiLayerPerceptron(layers, SigmoidCrossEntropyLoss(), Sigmoid())\n",
    "    best_mlp = None\n",
    "    best_f1 = 0.0\n",
    "    losses = []\n",
    "    batch_size = 16\n",
    "    for epoch in range(50):\n",
    "        mymlp.train_mode()\n",
    "        for i in range(n_batch):\n",
    "            _, loss = mymlp.forward(x_train[i*batch_size:(i+1)*batch_size ], \n",
    "                                    y_train[i*batch_size:(i+1)*batch_size ])\n",
    "            mymlp.backward()\n",
    "            _ = mymlp.optimize(lr)\n",
    "\n",
    "        train_acc, train_prec, train_rec, train_f1 = tester(\n",
    "            mymlp, x_train, y_train)\n",
    "        print(\"Epoch {} train_acc: {}, train_prec: {}, train_rec: {}, train_f1: {}\".format(\n",
    "            epoch, train_acc, train_prec, train_rec, train_f1))\n",
    "        if epoch % 1 == 0:\n",
    "            val_acc, val_prec, val_rec, val_f1 = tester(\n",
    "                mymlp, x_val, y_val)\n",
    "            print(\"Epoch {} val_acc: {}, val_prec: {}, val_rec: {}, val_f1: {}\".format(\n",
    "                epoch, val_acc, val_prec, val_rec, val_f1))\n",
    "\n",
    "            test_acc, test_prec, test_rec, test_f1 = tester(\n",
    "                mymlp, x_test, y_test)\n",
    "            print(\"Epoch {} test_acc: {}, test_prec: {}, test_rec: {}, test_f1: {}\".format(\n",
    "                epoch, test_acc, test_prec, test_rec, test_f1))\n",
    "\n",
    "            if val_f1 > best_f1:\n",
    "                best_mlp = deepcopy(mymlp)\n",
    "                best_f1 = val_f1\n",
    "                print(\"Saving as best model...\")\n",
    "                best_epno = epoch\n",
    "    #         if best_epno + 30 < epoch:\n",
    "    #             print('Validation not improving')\n",
    "    #             break\n",
    "\n",
    "\n",
    "        print('Loss', loss)\n",
    "        losses.append(loss)\n",
    "    lr_losses.append(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "offset = 15\n",
    "lr_losses = np.asarray(lr_losses)\n",
    "lr_losses = offset - lr_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for lr, run in zip(lrs, lr_losses):\n",
    "    plt.plot(run, label='lr={}'.format(lr))\n",
    "    plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross Entropy Error')\n",
    "plt.savefig('sentiment_loss_vs_lr.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
